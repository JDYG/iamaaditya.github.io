https://www.evernote.com/shard/s267/sh/64195d10-53b4-4312-8c5a-d10ab5138c36/22ce804ec6c8ab2b9ceb3096b8cd929e
Introduction

Started deep learning workshop - 2007 "A New Publishing Model in Computer Science"
Yann LeCun
ArXiv: dissociate dissemination & evaluation
Towards an open market for ideas
Recommend-Papers.org
Spatial Transformer Networks

Chair Andrew Ng By Max Jaderberg

Convnets are awesome but:

pooling is too simplistic -- need a better spatial invariance
missing a conditional spatial warping Can your network transform the data to a space expected by subsequent layers?
Need a better spatial training that can be trained just from gradients, no extra workshop
Spatial Transformer: + Localisation network + Grid Generator: produces a sampling grid + Differentiable Sampler

Can drop ST module anywhere in network:

Input -> ST -> CNN
Input -> CNN -> ST, etc.
Can do 3D Transformation too. 
- Code on github!

Batch Normalization

Sergey Iofe
Minimize loss over training data:

SGD
Follow gradient for mini-batches
Motivation: training with sigmoids

in saturated regions, sgd becomes very slow
happens even more for large networks cos fan-in to neurons are large, can easily move output to saturated region
some strategies for preventing:
careful initiliazation
Internal Covariate Shift

layer input distribs change during training
internal distrib changes, requires domain adaptation in outer distrib
reduce internal covariate shift to speed up training

normalization must participate in gradient optimization:

natural idea - compute E and Var as running averages:
doesnt work, blows up module
change the model architecture, not the optimizer
Normalize using mini-batch statistics (mini-batch mean & variance)

Can back prop with this batch-normalization
During inference, replace batch statistics with population statistics
During training, use mini-batch statistics, during inference, use population statistics
Batch Normalization does:

Reduce internal covariance shift
Speed up training
less dropout (BN serves as some form of regularizer)
easier initiliazation
ImageNet Classification SOA: Inception model with ReLU.

BN enables:

increased learning rate: like 30x
using saturating nonlinearities: logistic/sigmoid
mini-batch size of say ~32

Semi-Supervised Learning with Ladder Networks

By Harri Valpola
Unsupervised Learning - works well with unlabelled data
Ladder Networks learns inference in hierachical inherent models

Denoising Source Separation

Negentropy
Attention Mechanism Learning!

Semi-supervised is different from supervised-pretraining because both supervised and unsupervised learning occur concurrently. The supervised gives a rough idea as to where the unsupervised should be looking.
Deeply Learned Face Representation

Labeled Faces in the Wild (2007) Dataset
Learn Face Reps from:

face verification
id
multi-view reconstruction
DeepID

Generating new faces with different view points / poses
Adaptive, Articulate and Actionable Deep Learning

By Prof. Trevor Darell (inventors of Caffe)

Visual models adapt well to new domains.
articulates natural language descriptions:
compositional and lexically extensible
individuates objects
learnable action policies
Simultaneous Domain Adaptation: Train on source, adapt to target

Simplest thing to do: fine tune
Adapting across domains minimize discrepancy
have visual representations that are invariant across domains
can optimize for this, and get better features with minimal discrepancies across domains (add loss for domain confusion)
Task correlation transfer loss
soft domain loss 
Paper: Simultaneous Deep Transfer Across Domains
Articulate Deep Learning: Image Description

CNN + LSTM models : have the issues with training data (not enough contextual training examples)
"A dog sitting on a boat in the water"
Solve by coupling captioning model with a language model Paper: Deep Compositional Captioning
Compositionality in VisualQuestionAnswering - Neural Module Networks + Is there a red shape above a circle? 
Paper: Deep Compositional Question Answering

Paper: Natural Language Object Retrieval

given a scene and a natural language query, we want to localize the target object in the scene.
Spatial Context
Joint models that map pixels to actions: - Deep learning bringing different models and fields together

Panel Discussion

how do we expand the computational ability of networks?
unsupervised learning by itself is useless, unless there's a point to it
use the eventual task/purpose as a measure of the value -- for evaluating unsupervised model
hierachical reinforcement learning
Neural Turing Machines

Alex Graves
Basic idea: turn neural networks into 'differentialbe computers' by giving them read-write acces to external memory
NN ~ CPU, Numpers ~ Memory
Architecture: - Controller - neural network (recurrent or feedforward) - Heads - (attention) select portions of the memory to read/write to them - Memory - 'everything is differentiable'

Selective Attention:

want to focus on the parts of memory the network will read/write - need an introspective attention model
Addressing by content
Associative lookup
Addressing by Location
controller outputs a shift kernel
NTU can solve N-gram problems
Curriculum learning
Character-Aware Neural Language Models

language modelling
count-based language models
Neural Language Models:

word embedding vs one-hot encoding
issue: fundamental unit of information is still the word

separate embeddings for 'Â´trading', 'trade', 'trader'

how do you leverage sub-word information ?
use morphological segmenter as a preprocessing step (stemmers)
suggested idea: no explicit morphology, use characters directly
use similar idea as convnets
Network Architecture

Character Level Convnets (CharCNN)
Highway Network
RNN 
CharCNN word embeddings perform just as well as normal word embeddings
Highway Networks
Deep Reinforcement Learning for Robotics

Agent, environment, state, action, policy
Additional Challenges:

Stability
Credit Assignment
Exploration
Ways to tackle:

Policy Optimization
Gradient Estimate +
Policy Optimization + Trust Region Policy Optimization +

Reduction to Supervised Learning Q/V Value Function Methods

A Neural Network for Artistic Style

Each feature map, is just a filtered version of an image
CNN Image Reconstruction

At deeper layers, content information is preserved, but pixel information is lost.
Artistic Style Transfer

Optimization simultaneously preserves the content of the image, and the style of the painting.

Not just artistic style transfer, but general style transfer

Deep Generative Image Models using a Laplacian Adversarial Networks

parametric generative model of natural images
Generative Adversarial Networks
