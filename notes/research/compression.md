---
title: Model Compression
author: aaditya prakash
layout: page
dsq_thread_id:
- 
---


## Awesome lists

  * [Awesome Knowledge Distillation](https://github.com/dkozlov/awesome-knowledge-distillation/blob/master/README.md)
  * [Awesome model compression and acceleration](https://github.com/memoiry/Awesome-model-compression-and-acceleration)
  * [CNN Compression Survey](https://github.com/andyhahaha/Convolutional-Neural-Network-Compression-Survey)


## Reading list

### Compression

  * [Compressing Neural Networks using the Variational Information Bottleneck](https://arxiv.org/pdf/1802.10399.pdf)
  * [WEIGHTLESS: LOSSY WEIGHT ENCODING FOR DEEP NEURAL NETWORK COMPRESSION](https://arxiv.org/pdf/1711.04686.pdf)
  * [To prune, or not to prune: exploring the efficacy of pruning for model compression](https://arxiv.org/pdf/1710.01878.pdf)

### Pruning

  * [Dynamic Filter Networks](https://pdfs.semanticscholar.org/aba4/8504f4f9563eafa44e0cfb22e1345d767c80.pdf)
  * [Dynamic Steerable Blocks in Deep Residual Networks](https://arxiv.org/pdf/1706.00598.pdf)

### Knowledge Distillation

  * [Knowledge Distillation - Slides: Hinton](http://www.ttic.edu/dl/dark14.pdf)
  * [Gift from Knowledge Distillation](http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf)
