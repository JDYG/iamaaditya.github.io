<!DOCTYPE html>
<html>
  <head>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"> </script>
    <title>Pseudo loss function in distributed Adaboost – Aaditya Prakash (Adi) – Random Musings of Computer Vision grad student</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="$$L_{Hedge(\beta)}$$
" />
    <meta property="og:description" content="$$L_{Hedge(\beta)}$$
" />
    
    <meta name="author" content="Aaditya Prakash (Adi)" />

    
    <meta property="og:title" content="Pseudo loss function in distributed Adaboost" />
    <meta property="twitter:title" content="Pseudo loss function in distributed Adaboost" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Aaditya Prakash (Adi) - Random Musings of Computer Vision grad student" href="/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/profile_image.png" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">Aaditya Prakash (Adi)</a></h1>
            <p class="site-description">Random Musings of Computer Vision grad student</p>
          </div>

          <nav>
            <a href="/">Blog</a>
            <a href="/notes">Notes</a>
            <a href="/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>Pseudo loss function in distributed Adaboost</h1>

  <div class="entry">
    <h1>$$L_{Hedge(\beta)}$$</h1>

<p>This is the sketch of proof of correctness of <img src='http://s0.wp.com/latex.php?latex=L_%7BHedge%28%5Cbeta%29%7D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='L_{Hedge(\beta)} ' title='L_{Hedge(\beta)} ' class='latex' />, pseudo loss function, in the case of distributed boosting algorithm. It has been known that boosting of functions with each applicator with more than 0.5 accuracy is sufficient to guarantee lowest minimum accuracy in iterative pooling. A similar sketch is presented for the distributed case where the score from each stage is not shared across all the computing nodes. While this guarantees the correctness, it does not guarantee the convergence, which is still a highly sought after problem in distributed algorithm.</p>

<p>For people uninitiated in the problem, it is highly advised to <a href="http://www-users.cs.umn.edu/%7Ealeks/papers/kdd_01.pdf">read this paper</a> or for much simpler and faster read, <a href="http://www-users.cs.umn.edu/%7Ebanerjee/Teaching/Spring06/talks/Paper02Tim1.ppt">these slides</a>.</p>

<h2>Notations</h2>

<p><img src='http://s0.wp.com/latex.php?latex=w+%3D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='w = ' title='w = ' class='latex' /> weights Vector</p>

<p><img src='http://s0.wp.com/latex.php?latex=T+%3D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='T = ' title='T = ' class='latex' /> Number of iterations</p>

<p><img src='http://s0.wp.com/latex.php?latex=%5Cbeta&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='\beta' title='\beta' class='latex' /> = Parameter <img src='http://s0.wp.com/latex.php?latex=%5Cepsilon%5B0%2C1%5D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='\epsilon[0,1] ' title='\epsilon[0,1] ' class='latex' />  </p>

<p><img src='http://s0.wp.com/latex.php?latex=%5Cell%5E%7Bt%7D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='\ell^{t} ' title='\ell^{t} ' class='latex' />= Loss Vector for the <img src='http://s0.wp.com/latex.php?latex=t+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='t ' title='t ' class='latex' /> iteration (trials)</p>

<p>N = Total Number of weak learners</p>

<p><img src='http://s0.wp.com/latex.php?latex=%7BL_i%3D+%7D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='{L_i= } ' title='{L_i= } ' class='latex' />Strategy i&#8217;s cumulative loss = <img src='http://s0.wp.com/latex.php?latex=%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Cell_%7Bi%7D%5E%7Bt%7D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='\sum_{t=1}^{T}\ell_{i}^{t} ' title='\sum_{t=1}^{T}\ell_{i}^{t} ' class='latex' />  </p>

<p><img src='http://s0.wp.com/latex.php?latex=L_%7BA%7D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='L_{A} ' title='L_{A} ' class='latex' />= Algorithm A&#8217;s total cumulative loss = <img src='http://s0.wp.com/latex.php?latex=%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Cmathbf%7Bp%7D%5E%7Bt%7D.%5Cell%5E%7Bt%7D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='\sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t} ' title='\sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t} ' class='latex' /></p>

<p>&nbsp;</p>

<h2>Algorithm</h2>

<p>Consider the following Algorithm</p>

<p>Hedge(<img src='http://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='\beta ' title='\beta ' class='latex' />) :</p>

<p>Do for t = 1, 2, &#8230;, <em>T</em></p>

<ol>
<li>Choose Allocation</li>
</ol>

<p>\[<br>
\mathbf{p}^{t}=\frac{\mathbf{w}^{t}}{\sum_{i-1}^{N}w_{i}^{t}}<br>
\]</p>

<ol>
<li><p>Receive loss vector <img src='http://s0.wp.com/latex.php?latex=%5Cell%5E%7Bt%7D%5Cepsilon%5B0%2C1%5D%5E%7BN%7D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='\ell^{t}\epsilon[0,1]^{N} ' title='\ell^{t}\epsilon[0,1]^{N} ' class='latex' /> from environment</p></li>
<li><p>Suffer loss <img src='http://s0.wp.com/latex.php?latex=%5Cmathbf%7Bp%7D%5E%7Bt%7D.%5Cell%5E%7Bt%7D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='\mathbf{p}^{t}.\ell^{t} ' title='\mathbf{p}^{t}.\ell^{t} ' class='latex' /></p></li>
<li><p>Set the new weights</p></li>
</ol>

<p>\[<br>
w_{i}^{t+1}=w_{i}^{t}\beta^{\ell_{i}^{t}}<br>
\]</p>

<h2>To Prove</h2>

<p>\[<br>
ln(\sum_{i=1}^{N}w_{i}^{T+1})\leq-(1-\beta)L_{Hedge(\beta)}<br>
\]</p>

<h2>Proof</h2>

<p>From the Hedge(<img src='http://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='\beta ' title='\beta ' class='latex' />) function, we know that</p>

<p>\[<br>
\sum_{i=1}^{N}w_{i}^{t+1}=\sum_{i=1}^{N}w_{i}^{t}\beta^{\ell_{i}^{t}}<br>
\]</p>

<p>For <img src='http://s0.wp.com/latex.php?latex=%5Calpha%5Cgeq0+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='\alpha\geq0 ' title='\alpha\geq0 ' class='latex' /> and <img src='http://s0.wp.com/latex.php?latex=r%5Cepsilon%5B0%2C1%5D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='r\epsilon[0,1] ' title='r\epsilon[0,1] ' class='latex' />, by convexity we know that<br>
<img src='http://s0.wp.com/latex.php?latex=%5Calpha%5E%7Br%7D%5Cleq1-%281-%5Calpha%29r&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='\alpha^{r}\leq1-(1-\alpha)r' title='\alpha^{r}\leq1-(1-\alpha)r' class='latex' />, therefore above eqn can be rewritten<br>
as,</p>

<p>\[<br>
\sum_{i=1}^{N}w_{i}^{t+1}\leq\sum_{i=1}^{N}w_{i}^{t}(1-(1-\beta)\ell_{i}^{t})<br>
\]</p>

<p>\[<br>
\sum_{i=1}^{N}w_{i}^{t+1}\leq\left(\sum_{i=1}^{N}w_{i}^{t}\right)(1-(1-\beta)\mathbf{p}^{t}.\ell_{i}^{t})<br>
\]</p>

<p>For all the trials <img src='http://s0.wp.com/latex.php?latex=t%3D1%2C%5Cldots%2CT+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='t=1,\ldots,T ' title='t=1,\ldots,T ' class='latex' /></p>

<p>\[<br>
\sum_{i=1}^{N}w_{i}^{t+1}\leq\prod_{t=1}^{T}(1-(1-\beta)\mathbf{p}^{t}.\ell^{t}<br>
\]</p>

<p>(since <img src='http://s0.wp.com/latex.php?latex=1%2Bx%5Cleq+e%5E%7Bx%7D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='1+x\leq e^{x} ' title='1+x\leq e^{x} ' class='latex' /> )</p>

<p>\[<br>
\sum_{i=1}^{N}w_{i}^{t+1}\leq exp\left(-(1-\beta)\sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t}\right)<br>
\]</p>

<p>Taking log on both sides</p>

<p>\[<br>
ln\left(\sum_{i=1}^{N}w_{i}^{t+1}\right)\leq-(1-\beta)\sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t}<br>
\]</p>

<p>Taking negative sign and switching the equality</p>

<p>\[<br>
(1-\beta)\sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t}\leq ln\left(\sum_{i=1}^{N}w_{i}^{t+1}\right)<br>
\]</p>

<p>\[<br>
\sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t}\leq\frac{ln\left(\sum_{i=1}^{N}w_{i}^{t+1}\right)}{(1-\beta)}<br>
\]</p>

<p>Now from the definition of <img src='http://s0.wp.com/latex.php?latex=L_%7BA%7D+%3D+%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Cmathbf%7Bp%7D%5E%7Bt%7D.%5Cell%5E%7Bt%7D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='L_{A} = \sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t} ' title='L_{A} = \sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t} ' class='latex' />,<br>
since the algorithm here is Hedge(<img src='http://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='\beta ' title='\beta ' class='latex' />), where <img src='http://s0.wp.com/latex.php?latex=%5Cbeta+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='\beta ' title='\beta ' class='latex' />is the<br>
parameter, <img src='http://s0.wp.com/latex.php?latex=L_%7BA%7D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='L_{A} ' title='L_{A} ' class='latex' />, is called as <img src='http://s0.wp.com/latex.php?latex=L_%7BHedge%28%5Cbeta%29%7D+&#038;bg=ffffff&#038;fg=000000&#038;s=0' alt='L_{Hedge(\beta)} ' title='L_{Hedge(\beta)} ' class='latex' /></p>

<p>therefore,</p>

<p>\[<br>
L_{Hedge(\beta)}\leq\frac{ln\left(\sum_{i=1}^{N}w_{i}^{t+1}\right)}{(1-\beta)}<br>
\]</p>

<p style="text-align: right;">
  <span style="text-decoration: underline;">QED</span>
</p>

  </div>

  <div class="date">
    Written on December 10, 2014
  </div>

  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'iamaaditya';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:aprakash [at] brandeis . edu"><i class="svg-icon email"></i></a>


<a href="https://github.com/barryclark/jekyll-now"><i class="svg-icon github"></i></a>

<a href="https://www.linkedin.com/in/http://in.linkedin.com/pub/aaditya-prakash/38/533/684"><i class="svg-icon linkedin"></i></a>

<a href="/feed.xml"><i class="svg-icon rss"></i></a>
<a href="https://www.twitter.com/jekyllrb"><i class="svg-icon twitter"></i></a>


<a href="https://plus.google.com//u/0/100303074762902184969?rel=author"><i class="svg-icon googleplus"></i></a>
        </footer>
      </div>
    </div>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-70371377-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/2014/12/pseudo-loss-function-in-distributed-adaboost/',
		  'title': 'Pseudo loss function in distributed Adaboost'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
