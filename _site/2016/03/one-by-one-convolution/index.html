<!DOCTYPE html>
<html>
  <head>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"> </script>
    <link href='http://fonts.googleapis.com/css?family=Ovo' rel='stylesheet' type='text/css'>

    <title>One by One [ 1 x 1 ] Convolution - counter-intuitively useful – Aaditya Prakash (Adi) – Random Musings of Computer Vision grad student</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

    
    <meta name="description" content="Whenever I discuss or show GoogleNet architecture, one question always comes up - 
"Why 1x1 convolution ? Is it not redundant ?

" />
    <meta property="og:description" content="Whenever I discuss or show GoogleNet architecture, one question always comes up - 
"Why 1x1 convolution ? Is it not redundant ?

" />
    
    <meta name="author" content="Aaditya Prakash (Adi)" />

    
    <meta property="og:title" content="One by One [ 1 x 1 ] Convolution - counter-intuitively useful" />
    <meta property="twitter:title" content="One by One [ 1 x 1 ] Convolution - counter-intuitively useful" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/blog/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Aaditya Prakash (Adi) - Random Musings of Computer Vision grad student" href="/blog/feed.xml" />

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/blog/" class="site-avatar"><img src="https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/profile_image.jpg" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/blog/">Aaditya Prakash (Adi)</a></h1>
            <p class="site-description">Random Musings of Computer Vision grad student</p>
          </div>

          <nav>
            <a href="/blog/">Blog</a>
            <a href="/blog/research">Research</a>
            <a href="/blog/notes">Notes</a>
            <a href="/blog/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <article class="post">
  <h1>One by One [ 1 x 1 ] Convolution - counter-intuitively useful</h1>

  <div class="entry">
    <p>Whenever I discuss or show <a href="http://arxiv.org/pdf/1409.4842v1.pdf">GoogleNet architecture</a>, one question always comes up - <br /><br />
<strong><center>"Why 1x1 convolution ? Is it not redundant ?</center></strong></p>

<p><img src="https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/full_padding_no_strides_transposed.gif" alt="Convolution with Kernel of size 3x3" />
<img src="https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/full_padding_no_strides_transposed_small.gif" alt="Convolution with Kernel of size 1x1" /></p>

<p>left : <strong>Convolution with kernel of size 3x3</strong>               right : <strong>Convolution with kernel of size 1x1</strong></p>

<h2 id="simple-answer">Simple Answer</h2>

<p>Most simplistic explanation would be that 1x1 convolution leads to dimension reductionality. For example, an image of 200 x 200 with 50 features on convolution with 20 filters of 1x1 would result in size of 200 x 200 x 20.
But then again, is this is the best way to do dimensionality reduction in the convoluational neural network? What about the efficacy vs efficiency?</p>

<h2 id="complex-answer">Complex Answer</h2>

<h3 id="feature-transformation">Feature transformation</h3>
<p>Although 1x1 convolution is a ‘feature pooling’ technique, there is more to it than just sum pooling of features across various channels/feature-maps of a given layer. 
1x1 convolution acts like coordinate-dependent transformation in the filter space[<a href="https://plus.google.com/118431607943208545663/posts/2y7nmBuh2ar">1</a>]. It is important to note here that this transformation is strictly linear, but in most of application of 1x1 convolution, it is succeeded by a non-linear activation layer like ReLU. This transformation is learned through the (stochastic) gradient descent. But an important distinction is that it suffers with less over-fitting due to smaller kernel size (1x1).</p>

<h3 id="deeper-network">Deeper Network</h3>

<p>One by One convolution was first introduced in this paper titled <a href="http://arxiv.org/pdf/1312.4400v3.pdf">Network in Network</a>. In this paper, the author’s goal was to generate a deeper network without simply stacking more layers. It replaces few filters with a smaller perceptron layer with mixture of 1x1 and 3x3 convolutions. In a way, it can be seen as “going wide” instead of “deep”, but it should be noted that in machine learning terminology, ‘going wide’ is often meant as adding more data to the training. Combination of 1x1 (x F) convolution is mathematically equivalent to a multi-layer perceptron.[<a href="https://www.reddit.com/r/MachineLearning/comments/3oln72/1x1_convolutions_why_use_them/cvyxood">2</a>].</p>

<p><strong>Inception Module</strong></p>

<p>In GoogLeNet architecture, 1x1 convolution is used for two purposes</p>

<ul>
  <li>To make network deep by adding an “inception module” like Network in Network paper, as described above.</li>
  <li>To reduce the dimensions inside this “inception module”.</li>
  <li>To add more non-linearity by having ReLU immediately after every 1x1 convolution.</li>
</ul>

<p>Here is the scresnshot from the paper, which elucidates above points :</p>

<p><img src="https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/inception_1x1.png" alt="1x1 convolutions in GoogLeNet" />
<strong><center>1x1 convolutions in GoogLeNet</center></strong></p>

<p>It can be seen from the image on the right, that 1x1 convolutions (in yellow), are specially used before 3x3 and 5x5 convolution to reduce the dimensions. It should be noted that a two step convolution operation can always to combined into one, but in this case and in most other deep learning networks, convolutions are followed by non-linear activation and hence convolutions are no longer linear operators and cannot be combined.</p>

<p>In designing such a network, it is important to note that initial convolution kernel should be of size larger than 1x1 to have a receptive field capable of capturing locally spatial information. According to the NIN paper, 1x1 convolution is equivalent to cross-channel parametric pooling layer. From the paper - “This cascaded cross channel parameteric pooling structure allows complex and learnable interactions of cross channel information”.</p>

<p>Cross channel information learning (cascaded 1x1 convolution) is biologically inspired because human visual cortex have receptive fields (kernels) tuned to different orientation. For e.g</p>

<p><img src="https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/RotBundleFiltersListPlot3D.gif" alt="different orientation tuned receptive field profiles in the human visual cortex" /></p>

<p><strong>Different orientation tuned receptive field profiles in the human visual cortex</strong> <a href="http://bmia.bmt.tue.nl/education/courses/fev/course/notebooks/Convolution.html">Source</a></p>

<h2 id="more-uses">More Uses</h2>

<ul>
  <li>1x1 Convolution can be combined with Max pooling</li>
</ul>

<p><img src="https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/numerical_max_pooling.gif" alt="Pooling with 1x1 convolution" />
   <strong>Pooling with 1x1 convolution</strong>
  <br /></p>

<ul>
  <li>1x1 Convolution with higher strides leads to even more redution in data by decreasing resolution, while losing very little non-spatially correlated information.</li>
</ul>

<p><img src="https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/no_padding_strides.gif" alt="1x1 convolution with strides" />
   <strong>1x1 convolution with strides</strong>
   <br /></p>

<ul>
  <li>Replace fully connected layers with 1x1 convolutions as Yann LeCun believes they are the same -
&gt; In Convolutional Nets, there is no such thing as “fully-connected layers”. There are only convolution layers with 1x1 convolution kernels and a full connection table.
– <a href="https://www.facebook.com/yann.lecun/posts/10152820758292143">Yann LeCun</a>
  <br /></li>
</ul>

<p><em>Convolution gif images generated using <a href="https://github.com/vdumoulin/conv_arithmetic">this wonderful code</a>, more images on 1x1 convolutions and 3x3 convolutions can be</em> <a href="http://gpgpu.cs-i.brandeis.edu/convolution_images/">found here</a></p>


  </div>

  <div class="date">
    Written on March 25, 2016
  </div>

  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'iamaaditya';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</article>

    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:aprakash [at] brandeis . edu"><i class="svg-icon email"></i></a>


<a href="https://github.com/iamaaditya"><i class="svg-icon github"></i></a>

<a href="https://www.linkedin.com/in/aaditya-prakash-68453338"><i class="svg-icon linkedin"></i></a>

<a href="/blog/feed.xml"><i class="svg-icon rss"></i></a>
<a href="https://www.twitter.com/aaditya_prakash"><i class="svg-icon twitter"></i></a>


<a href="https://plus.google.com//u/0/100303074762902184969?rel=author"><i class="svg-icon googleplus"></i></a>
          <br />
          Built using Jekyll. No copyright held. Aaditya Prakash.
        </footer>
      </div>
    </div>

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-70371377-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/2016/03/one-by-one-convolution/',
		  'title': 'One by One [ 1 x 1 ] Convolution - counter-intuitively useful'
		});
	</script>
	<!-- End Google Analytics -->


  </body>
</html>
