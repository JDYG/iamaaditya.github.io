<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aaditya Prakash (Adi)</title>
    <description>Random Musings of Computer Vision grad student</description>
    <link>http://iamaaditya.github.io</link>
    <atom:link href="http://iamaaditya.github.io/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Visual Question Answering Demo in Python Notebook</title>
        <description>&lt;p&gt;This is an online demo with explanation and tutorial on Visual Question
Answering. This is no not a naive or hello-world model, this model returns close
to state-of-the-art without using any attention models, memory netorks (other
than LSTM) and fine-tuning, which are essential recipe for current best results.&lt;/p&gt;

&lt;p&gt;I have tried to explain different parts, and their choices. This is meant to be
an interactive tutorial, feel free to change the model parameters and
experiment. If you have latest graphics card, execution time should be within a
minute.&lt;/p&gt;

&lt;p&gt;All the files required to run this ipython notebook can be obtained from
## &lt;a href=&quot;https://github.com/iamaaditya/VQA_Demo&quot;&gt;https://github.com/iamaaditya/VQA_Demo&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;load-the-libraries&quot;&gt;Load the libraries&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;what?&lt;/span&gt;
  &lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;~~~~~~~~
Here comes some code.
~~~~~~~&lt;/p&gt;

&lt;p&gt;```{.python .input  n=30}&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;argparse&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;spacy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_from_json&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.optimizers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.externals&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;joblib&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;h2 id=&quot;load-the-models-and-weights-files&quot;&gt;Load the models and weights files&lt;/h2&gt;
&lt;p&gt;This does not load the models yet, but we are providing the files&lt;/p&gt;

&lt;p&gt;```{.python .input  n=1}
# File paths for the model, all of these except the CNN Weights are 
# provided in the repo, See the models/CNN/README.md to download VGG weights
VQA_model_file_name      = ‘models/VQA/VQA_MODEL.json’
VQA_weights_file_name   = ‘models/VQA/VQA_MODEL_WEIGHTS.hdf5’
label_encoder_file_name  = ‘models/VQA/FULL_labelencoder_trainval.pkl’
CNN_weights_file_name   = ‘models/CNN/vgg16_weights.h5’&lt;/p&gt;

&lt;p&gt;```&lt;/p&gt;

&lt;h2 id=&quot;model-idea&quot;&gt;Model Idea&lt;/h2&gt;
&lt;p&gt;This uses a classical CNN-LSTM  model like shown below, where Image features and
language features are computed separetely and combined together and a multi-
layer perceptron is trained on the combined features.&lt;/p&gt;

&lt;p&gt;Similar models have been presented at following links, this work takes ideas
from them.
1. &lt;a href=&quot;https://github.com/abhshkdz/neural-vqa&quot;&gt;https://github.com/abhshkdz/neural-vqa&lt;/a&gt;
2. &lt;a href=&quot;https://github.com/avisingh599/visual-qa&quot;&gt;https://github.com/avisingh599/visual-qa&lt;/a&gt;
3. https://github.com/VT-vision-lab/VQA_LSTM_CNN&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://camo.githubusercontent.com/c369855a52126470fe0435552166b84a387 15489/687474703a2f2f692e696d6775722e636f6d2f555841506c71652e706e67&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;image-features&quot;&gt;Image features&lt;/h1&gt;

&lt;h2 id=&quot;pretrained-vgg-net-vgg-16&quot;&gt;Pretrained VGG Net (VGG-16)&lt;/h2&gt;

&lt;p&gt;While VGG Net is not the best CNN model for image features, GoogLeNet (winner
2014) and ResNet (winner 2015) have superior classification scores, but VGG Net
is very versatile, simple, relatively small and more importantly portable to
use.&lt;/p&gt;

&lt;p&gt;For reference here is the VGG 16 performance on ILSVRC-2012
&lt;img src=&quot;http://www.robots.ox.ac.uk/~vgg/research/very_deep/images/table_ILSVRC .png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;compile-the-model&quot;&gt;Compile the model&lt;/h2&gt;

&lt;p&gt;```{.python .input  n=32}
def get_image_model(CNN_weights_file_name):
    ‘’’ Takes the CNN weights file, and returns the VGG model update 
    with the weights. Requires the file VGG.py inside models/CNN ‘’’
    from models.CNN.VGG import VGG_16
    image_model = VGG_16(CNN_weights_file_name)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# this is standard VGG 16 without the last two layers
sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
# one may experiment with &quot;adam&quot; optimizer, but the loss function for
# this kind of task is pretty standard
image_model.compile(optimizer=sgd, loss=&#39;categorical_crossentropy&#39;)
return image_model ```
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;plot-the-model&quot;&gt;Plot the Model&lt;/h2&gt;

&lt;p&gt;Keras has a function which allows you to visualize the model in block diagram.
Let’s do it !&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.python&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.input&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=35&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;keras.utils.visualize_util&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;model_vgg&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;get_image_model(CNN_weights_file_name)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;plot(model_vgg,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;to_file=&#39;model_vgg.png&#39;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;model_vgg.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;extract-image-features&quot;&gt;Extract Image features&lt;/h2&gt;

&lt;p&gt;Extracting image features involves, taking a raw image, and running it through
the model, until we reach the last layer. In this case our model is not 100%
same as VGG Net, because we are not going to use the last two layer of the VGG.
It is becuase the last layer of VGG Net is a 1000 way soft max and the second
last layer is the Dropout.&lt;/p&gt;

&lt;p&gt;Thus we are extracting the 4096 Dimension image features from VGG-16&lt;/p&gt;

&lt;p&gt;```{.python .input  n=7}
def get_image_features(image_file_name, CNN_weights_file_name):
    ‘’’ Runs the given image_file to VGG 16 model and returns the 
    weights (filters) as a 1, 4096 dimension vector ‘’’
    image_features = np.zeros((1, 4096))
    # Magic_Number = 4096  &amp;gt; Comes from last layer of VGG Model&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Since VGG was trained as a image of 224x224, every new image
# is required to go through the same transformation
im = cv2.resize(cv2.imread(image_file_name), (224, 224))
im = im.transpose((2,0,1)) # convert the image to RGBA


# this axis dimension is required becuase VGG was trained on a dimension
# of 1, 3, 224, 224 (first axis is for the batch size
# even though we are using only one image, we have to keep the dimensions consistent
im = np.expand_dims(im, axis=0) 

image_features[0,:] = get_image_model(CNN_weights_file_name).predict(im)[0]
return image_features ```
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h1 id=&quot;word-embeddings&quot;&gt;Word Embeddings&lt;/h1&gt;

&lt;p&gt;The question has to be converted into some form of word embeddings. Most popular
is Word2Vec whereas these days state of the art uses &lt;a href=&quot;https://github.com/ryankiros/skip-thoughts&quot;&gt;skip-thought
vectors&lt;/a&gt; or [postional
encodings](https://en.wikipedia.org/wiki/Encoding_(memory)#Sequence_memory}.&lt;/p&gt;

&lt;p&gt;We will use Word2Vec from Standford called
&lt;a href=&quot;http://nlp.stanford.edu/projects/glove/&quot;&gt;Glove&lt;/a&gt;. Glove reduces a given token
into a 300 dimensional representation.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.python&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.input&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;def&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;get_question_features(question):&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&#39;&#39;&#39;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;For&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;given&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;question,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;unicode&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;string,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;returns&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;the&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;timeseris&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;each&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;(token)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;transformed&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;into&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;dimension&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;representation&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;calculated&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;using&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Glove&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;Vector&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&#39;&#39;&#39;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;word_embeddings&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;spacy.load(&#39;en&#39;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;vectors=&#39;en_glove_cc_&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;m_vectors&#39;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;word_embeddings(question)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;question_tensor&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;np.zeros((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;len(tokens),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;xrange(len(tokens)):&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;question_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;tokens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.vector&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;question_tensor&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;try-the-embeddigns&quot;&gt;Try the embeddigns&lt;/h2&gt;

&lt;p&gt;Let’s see the embeddings, and their usage with sample words like this -
1. Obama
2. Putin
3. Banana
4. Monkey&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.python&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.input&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=23&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;word_embeddings&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;spacy.load(&#39;en&#39;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;vectors=&#39;en_glove_cc_&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;300&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;m_vectors&#39;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.python&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.input&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=24&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;obama&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;word_embeddings(u&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;obama&quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;putin&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;word_embeddings(u&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;putin&quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;banana&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;word_embeddings(u&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;banana&quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;monkey&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;word_embeddings(u&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;monkey&quot;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.python&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.input&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;obama.similarity(putin)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.json&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.output&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;text/plain&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0.43514112534149385&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;execution_count&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;metadata&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;output_type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;execute_result&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.python&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.input&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=26&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;obama.similarity(banana)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.json&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.output&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=26&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;text/plain&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0.17831375020636123&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;execution_count&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;26&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;metadata&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;output_type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;execute_result&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.python&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.input&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=27&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;banana.similarity(monkey)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.json&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.output&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=27&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
   &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;text/plain&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0.45207779162154438&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;execution_count&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;27&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;metadata&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;output_type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;execute_result&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;As we can see, obama and putin are very similar in representation than obama and
banana. This shows you there is some semantic knowledge of the tokens embedded
in the 300 dimensional representation. We can do cool arithematics with these
word2vec like ‘Queen’ - ‘King’ + ‘Boy’ = ‘Girl’. See &lt;a href=&quot;http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a
-thousand-vectors/&quot;&gt;this blog
post&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h2 id=&quot;vqa-model&quot;&gt;VQA Model&lt;/h2&gt;

&lt;p&gt;VQA is a simple model which combines features from Image and Word Embeddings and
runs a multiple layer&lt;/p&gt;

&lt;p&gt;```{.python .input  n=36}
def get_VQA_model(VQA_model_file_name, VQA_weights_file_name):
    ‘’’ Given the VQA model and its weights, compiles and returns the model ‘’’&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# thanks the keras function for loading a model from JSON, this becomes
# very easy to understand and work. Alternative would be to load model
# from binary like cPickle but then model would be obfuscated to users
vqa_model = model_from_json(open(VQA_model_file_name).read())
vqa_model.load_weights(VQA_weights_file_name)
vqa_model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;rmsprop&#39;)
return vqa_model ```
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.python&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.input&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=38&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;from&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;keras.utils.visualize_util&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;model_vqa&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;get_VQA_model(VQA_model_file_name,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;VQA_weights_file_name)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;plot(model_vqa,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;to_file=&#39;model_vqa.png&#39;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;model_vqa.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As it can be seen above the model also runs a 3 layered LSTM on the word
embeddings. To get a naive result it is sufficient to feed the word embeddings
directly to the merge layer, but as mentioned above model is gives close to the
state-of-the-art results.&lt;/p&gt;

&lt;p&gt;Also, four layers of fully conntected layers might not be required to achieve a
good enough results. But I settled on this model after some experimentation, and
their results beat few layers.&lt;/p&gt;

&lt;h2 id=&quot;asketh-away-&quot;&gt;Asketh Away !&lt;/h2&gt;

&lt;p&gt;Let’s give a test image and a question&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.python&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.input&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=69&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;image_file_name&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&#39;test.jpg&#39;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;question&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;What vechile is in the picture?&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;h1 id=&quot;center-what-vechile-is-in-the-picture--center&quot;&gt;&lt;center&gt; What vechile is in the picture ? &lt;/center&gt;&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;test.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.python&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.input&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=43&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;the&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;image_features&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;get_image_features(image_file_name,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;CNN_weights_file_name)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.python&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.input&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=44&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;the&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;question&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;question_features&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;get_question_features(question)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;```{.python .input  n=46}
y_output = model_vqa.predict([question_features, image_features])&lt;/p&gt;

&lt;h1 id=&quot;this-task-here-is-represented-as-a-classification-into-a-1000-top-answers&quot;&gt;This task here is represented as a classification into a 1000 top answers&lt;/h1&gt;
&lt;p&gt;# this means some of the answers were not part of trainng and thus would 
# not show up in the result.
# These 1000 answers are stored in the sklearn Encoder class
labelencoder = joblib.load(label_encoder_file_name)
for label in reversed(np.argsort(y_output)[0,-5:]):
    print str(round(y_output[0,label]*100,2)).zfill(5), “% “, labelencoder.inverse_transform(label)
```&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.json&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.output&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=46&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;stdout&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;output_type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;stream&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;78.32 %  train\n01.11 %  truck\n00.98 %  passenger\n00.95 %  fire truck\n00.68 %  bus\n&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;I am copying the output of the previous command, so that you can validate if
your results are same as mine.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;78.32 %  train&lt;/strong&gt; &lt;br /&gt;
01.11 %  truck &lt;br /&gt;
00.98 %  passenger &lt;br /&gt;
00.95 %  fire truck &lt;br /&gt;
00.68 %  bus &lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;demo-with-image-url&quot;&gt;Demo with image URL&lt;/h1&gt;

&lt;p&gt;Since cv2.imread cannot read an image from URL we will have to change our
fucntion &lt;code class=&quot;highlighter-rouge&quot;&gt;get_image_features&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;```{.python .input  n=49}
def get_image_features(image_file_name, CNN_weights_file_name):
    ‘’’ Runs the given image_file to VGG 16 model and returns the 
    weights (filters) as a 1, 4096 dimension vector ‘’’
    image_features = np.zeros((1, 4096))&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from skimage import io
# if you would rather not install skimage, then use cv2.VideoCapture which surprisingly can read from url
# see this SO answer http://answers.opencv.org/question/16385/cv2imread-a-url/?answer=16389#post-id-16389
im = cv2.resize(io.imread(image_file_name), (224, 224))
im = im.transpose((2,0,1)) # convert the image to RGBA


# this axis dimension is required becuase VGG was trained on a dimension
# of 1, 3, 224, 224 (first axis is for the batch size
# even though we are using only one image, we have to keep the dimensions consistent
im = np.expand_dims(im, axis=0) 

image_features[0,:] = get_image_model(CNN_weights_file_name).predict(im)[0]
return image_features ```
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.python&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.input&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=61&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;image_file_name&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://www.newarkhistory.com/indparksoccerkids.jpg&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;the&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;image_features&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;get_image_features(image_file_name,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;CNN_weights_file_name)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Feel free to change that url to any valid image, it can be any image format.
Also try to use websites which have higher bandwidth&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.newarkhistory.com/indparksoccerkids.jpg&quot; /&gt;
# &lt;center&gt; What are they playing? &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;```{.python .input  n=62}
question = u”What are they playing?”&lt;/p&gt;

&lt;h1 id=&quot;get-the-question-features&quot;&gt;get the question features&lt;/h1&gt;
&lt;p&gt;question_features = get_question_features(question)
```&lt;/p&gt;

&lt;p&gt;```{.python .input  n=63}
y_output = model_vqa.predict([question_features, image_features])&lt;/p&gt;

&lt;p&gt;labelencoder = joblib.load(label_encoder_file_name)
for label in reversed(np.argsort(y_output)[0,-5:]):
    print str(round(y_output[0,label]*100,2)).zfill(5), “% “, labelencoder.inverse_transform(label)
```&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.json&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.output&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=63&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;stdout&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;output_type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;stream&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;40.52 %  tennis\n28.45 %  soccer\n17.88 %  baseball\n11.67 %  frisbee\n00.15 %  football\n&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;
&lt;p&gt;Copying the result to validate your output.&lt;/p&gt;

&lt;p&gt;**40.52 %  tennis **&lt;br /&gt;
28.45 %  soccer &lt;br /&gt;
17.88 %  baseball &lt;br /&gt;
11.67 %  frisbee &lt;br /&gt;
00.15 %  football &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, it got this wrong, but you can see why it could be harder to
guess soccer and easier to guess tennis, lack of soccer ball and double lines at
the edge.&lt;/p&gt;

&lt;p&gt;Let’s ask another question for the same image.&lt;/p&gt;

&lt;p&gt;```{.python .input  n=67}
question = u”Are they playing soccer?”&lt;/p&gt;

&lt;h1 id=&quot;get-the-question-features-1&quot;&gt;get the question features&lt;/h1&gt;
&lt;p&gt;question_features = get_question_features(question)
```&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.newarkhistory.com/indparksoccerkids.jpg&quot; /&gt;
# &lt;center&gt; Are they playing soccer? &lt;/center&gt;&lt;/p&gt;

&lt;p&gt;```{.python .input  n=68}
y_output = model_vqa.predict([question_features, image_features])&lt;/p&gt;

&lt;p&gt;labelencoder = joblib.load(label_encoder_file_name)
for label in reversed(np.argsort(y_output)[0,-5:]):
    print str(round(y_output[0,label]*100,2)).zfill(5), “% “, labelencoder.inverse_transform(label)
```&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.json&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;.output&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;n=68&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;stdout&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;output_type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;stream&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;93.15 %  yes\n06.42 %  no\n00.02 %  right\n00.01 %  left\n000.0 %  man\n&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;result-1&quot;&gt;Result&lt;/h2&gt;

&lt;p&gt;**93.15 %  yes **&lt;br /&gt;
06.42 %  no &lt;br /&gt;
00.02 %  right &lt;br /&gt;
00.01 %  left &lt;br /&gt;
000.0 %  man &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, similar information about a Yes/No question illicits different
response, or should I say correct response. This is an impertinent problem with
&lt;code class=&quot;highlighter-rouge&quot;&gt;classification&lt;/code&gt; tasks.&lt;/p&gt;

&lt;p&gt;Feel free to experiment with different types of questions, &lt;code class=&quot;highlighter-rouge&quot;&gt;count&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;color&lt;/code&gt;,
&lt;code class=&quot;highlighter-rouge&quot;&gt;location&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;More interesting results are obtained when one takes a different crop of a
image, instead of just scaling it to 224x224. This is again becuase we extract
only the top level features of CNN model which was trained to classify one
object in the image.&lt;/p&gt;
</description>
        <pubDate>Mon, 04 Apr 2016 00:00:00 -0400</pubDate>
        <link>http://iamaaditya.github.io/2016/03/visual-question-answering-demo-in-python-notebook/</link>
        <guid isPermaLink="true">http://iamaaditya.github.io/2016/03/visual-question-answering-demo-in-python-notebook/</guid>
      </item>
    
      <item>
        <title>Comparison of Visual Question Answering Models</title>
        <description>&lt;p&gt;There has been a resur&lt;/p&gt;

&lt;h1 id=&quot;memory&quot;&gt;Memory&lt;/h1&gt;

&lt;h1 id=&quot;attention-models-vs-non-attention-models&quot;&gt;Attention models vs non-attention models&lt;/h1&gt;

&lt;h1 id=&quot;lstm-vs-gru&quot;&gt;LSTM vs GRU&lt;/h1&gt;

&lt;h1 id=&quot;episodic-memory-vs-semantiv-memory&quot;&gt;Episodic memory vs Semantiv memory&lt;/h1&gt;

&lt;h1 id=&quot;no-bidirectinal-lstm&quot;&gt;No bidirectinal LSTM&lt;/h1&gt;

&lt;h1 id=&quot;activation-functions-used&quot;&gt;Activation functions used&lt;/h1&gt;

&lt;h1 id=&quot;choice-of-activations&quot;&gt;Choice of Activations&lt;/h1&gt;

&lt;h1 id=&quot;use-of-batch-normalization&quot;&gt;Use of Batch Normalization&lt;/h1&gt;

</description>
        <pubDate>Sat, 26 Mar 2016 00:00:00 -0400</pubDate>
        <link>http://iamaaditya.github.io/2016/03/comparison-of-visual-question-answering-models/</link>
        <guid isPermaLink="true">http://iamaaditya.github.io/2016/03/comparison-of-visual-question-answering-models/</guid>
      </item>
    
      <item>
        <title>One by One [ 1 x 1 ] Convolution - counter-intuitively useful</title>
        <description>&lt;p&gt;Whenever I discuss or show &lt;a href=&quot;http://arxiv.org/pdf/1409.4842v1.pdf&quot;&gt;GoogleNet architecture&lt;/a&gt;, one question always comes up - &lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;&lt;center&gt;&quot;Why 1x1 convolution ? Is it not redundant ?&lt;/center&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/full_padding_no_strides_transposed.gif&quot; alt=&quot;Convolution with Kernel of size 3x3&quot; /&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/full_padding_no_strides_transposed_small.gif&quot; alt=&quot;Convolution with Kernel of size 1x1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;left : &lt;strong&gt;Convolution with kernel of size 3x3&lt;/strong&gt;               right : &lt;strong&gt;Convolution with kernel of size 1x1&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;simple-answer&quot;&gt;Simple Answer&lt;/h2&gt;

&lt;p&gt;Most simplistic explanation would be that 1x1 convolution leads to dimension reductionality. For example, an image of 200 x 200 with 50 features on convolution with 20 filters of 1x1 would result in size of 200 x 200 x 20.
But then again, is this is the best way to do dimensionality reduction in the convoluational neural network? What about the efficacy vs efficiency?&lt;/p&gt;

&lt;h2 id=&quot;complex-answer&quot;&gt;Complex Answer&lt;/h2&gt;

&lt;h3 id=&quot;feature-transformation&quot;&gt;Feature transformation&lt;/h3&gt;
&lt;p&gt;Although 1x1 convolution is a ‘feature pooling’ technique, there is more to it than just sum pooling of features across various channels/feature-maps of a given layer. 
1x1 convolution acts like coordinate-dependent transformation in the filter space[&lt;a href=&quot;https://plus.google.com/118431607943208545663/posts/2y7nmBuh2ar&quot;&gt;1&lt;/a&gt;]. It is important to note here that this transformation is strictly linear, but in most of application of 1x1 convolution, it is succeeded by a non-linear activation layer like ReLU. This transformation is learned through the (stochastic) gradient descent. But an important distinction is that it suffers with less over-fitting due to smaller kernel size (1x1).&lt;/p&gt;

&lt;h3 id=&quot;deeper-network&quot;&gt;Deeper Network&lt;/h3&gt;

&lt;p&gt;One by One convolution was first introduced in this paper titled &lt;a href=&quot;http://arxiv.org/pdf/1312.4400v3.pdf&quot;&gt;Network in Network&lt;/a&gt;. In this paper, the author’s goal was to generate a deeper network without simply stacking more layers. It replaces few filters with a smaller perceptron layer with mixture of 1x1 and 3x3 convolutions. In a way, it can be seen as “going wide” instead of “deep”, but it should be noted that in machine learning terminology, ‘going wide’ is often meant as adding more data to the training. Combination of 1x1 (x F) convolution is mathematically equivalent to a multi-layer perceptron.[&lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/3oln72/1x1_convolutions_why_use_them/cvyxood&quot;&gt;2&lt;/a&gt;].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inception Module&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In GoogLeNet architecture, 1x1 convolution is used for two purposes&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To make network deep by adding an “inception module” like Network in Network paper, as described above.&lt;/li&gt;
  &lt;li&gt;To reduce the dimensions inside this “inception module”.&lt;/li&gt;
  &lt;li&gt;To add more non-linearity by having ReLU immediately after every 1x1 convolution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is the scresnshot from the paper, which elucidates above points :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/inception_1x1.png&quot; alt=&quot;1x1 convolutions in GoogLeNet&quot; /&gt;
&lt;strong&gt;&lt;center&gt;1x1 convolutions in GoogLeNet&lt;/center&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It can be seen from the image on the right, that 1x1 convolutions (in yellow), are specially used before 3x3 and 5x5 convolution to reduce the dimensions. It should be noted that a two step convolution operation can always to combined into one, but in this case and in most other deep learning networks, convolutions are followed by non-linear activation and hence convolutions are no longer linear operators and cannot be combined.&lt;/p&gt;

&lt;p&gt;In designing such a network, it is important to note that initial convolution kernel should be of size larger than 1x1 to have a receptive field capable of capturing locally spatial information. According to the NIN paper, 1x1 convolution is equivalent to cross-channel parametric pooling layer. From the paper - “This cascaded cross channel parameteric pooling structure allows complex and learnable interactions of cross channel information”.&lt;/p&gt;

&lt;p&gt;Cross channel information learning (cascaded 1x1 convolution) is biologically inspired because human visual cortex have receptive fields (kernels) tuned to different orientation. For e.g&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/RotBundleFiltersListPlot3D.gif&quot; alt=&quot;different orientation tuned receptive field profiles in the human visual cortex&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Different orientation tuned receptive field profiles in the human visual cortex&lt;/strong&gt; &lt;a href=&quot;http://bmia.bmt.tue.nl/education/courses/fev/course/notebooks/Convolution.html&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;more-uses&quot;&gt;More Uses&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;1x1 Convolution can be combined with Max pooling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/numerical_max_pooling.gif&quot; alt=&quot;Pooling with 1x1 convolution&quot; /&gt;
   &lt;strong&gt;Pooling with 1x1 convolution&lt;/strong&gt;
  &lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1x1 Convolution with higher strides leads to even more redution in data by decreasing resolution, while losing very little non-spatially correlated information.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/conv_arithmetic/no_padding_strides.gif&quot; alt=&quot;1x1 convolution with strides&quot; /&gt;
   &lt;strong&gt;1x1 convolution with strides&lt;/strong&gt;
   &lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Replace fully connected layers with 1x1 convolutions as Yann LeCun believes they are the same -
&amp;gt; In Convolutional Nets, there is no such thing as “fully-connected layers”. There are only convolution layers with 1x1 convolution kernels and a full connection table.
– &lt;a href=&quot;https://www.facebook.com/yann.lecun/posts/10152820758292143&quot;&gt;Yann LeCun&lt;/a&gt;
  &lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Convolution gif images generated using &lt;a href=&quot;https://github.com/vdumoulin/conv_arithmetic&quot;&gt;this wonderful code&lt;/a&gt;, more images on 1x1 convolutions and 3x3 convolutions can be&lt;/em&gt; &lt;a href=&quot;http://gpgpu.cs-i.brandeis.edu/convolution_images/&quot;&gt;found here&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Fri, 25 Mar 2016 00:00:00 -0400</pubDate>
        <link>http://iamaaditya.github.io/2016/03/one-by-one-convolution/</link>
        <guid isPermaLink="true">http://iamaaditya.github.io/2016/03/one-by-one-convolution/</guid>
      </item>
    
      <item>
        <title>Deep Learning software installation guide on fresh Ubuntu</title>
        <description>&lt;h1 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Basic
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#upgrade-the-ubuntu&quot;&gt;Upgrade the Ubuntu&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#raid-1&quot;&gt;RAID 1&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#partition-and-mount-the-harddrives&quot;&gt;Partition and mount the harddrives&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mounting&quot;&gt;Mounting&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#install-essentials-extras-git-zsh&quot;&gt;Install Essentials, Extras, Git, Zsh&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#python&quot;&gt;Python&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#python-and-libs&quot;&gt;Python and Libs&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#highend-computing&quot;&gt;Highend computing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#python3-and-ipython-jupyter&quot;&gt;Python3 and Ipython (Jupyter)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nvidia&quot;&gt;NVIDIA&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#install-nvidia-drivers&quot;&gt;Install NVIDIA Drivers&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#install-cuda&quot;&gt;Install CUDA&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#install-cudnn&quot;&gt;Install CuDNN&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#image-processing-and-computer-vision&quot;&gt;Image processing and Computer Vision&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#install-opencv&quot;&gt;Install OpenCV&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ffmpeg&quot;&gt;FFMPEG&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#imagemagick&quot;&gt;ImageMagick&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#install-gpu-libraries&quot;&gt;Install GPU Libraries&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#theano&quot;&gt;Theano&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#tensorflow&quot;&gt;Tensorflow&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#keras&quot;&gt;Keras&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lasagne&quot;&gt;Lasagne&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#torch&quot;&gt;Torch&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#caffe&quot;&gt;Caffe&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#note&quot;&gt;Note&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#requirements&quot;&gt;Requirements&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#issues&quot;&gt;Issues&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#optional&quot;&gt;Optional&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#remove-unnecessary-ubuntu-folders&quot;&gt;Remove unnecessary Ubuntu folders&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#customization&quot;&gt;Customization&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recently I assembled a machine with 4 GPU (Titan X), a clone of &lt;a href=&quot;https://developer.nvidia.com/devbox&quot;&gt;NVIDIA DevBox&lt;/a&gt;. There are few other blog posts which describe the hardware guide, so I will not go into the same detail. Please refer &lt;a href=&quot;https://www.facebook.com/notes/chris-lengerich/build-your-own-nvidia-devbox/10152999419281541&quot;&gt;this&lt;/a&gt;. 
The actual list of parts I bought can be found &lt;a href=&quot;https://pcpartpicker.com/user/iamaaditya/saved/LPmZxr&quot;&gt; at PCPart Picker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;However this blog post is about the software guide. Although, most of the researchers or grad students like me will have their own custom requirement I thought I should share my software installation guide, for someone who might be new and would benefit from some of these. I have had to do this process couple of times now, and I have learned from my mistakes.&lt;/p&gt;

&lt;p&gt;NOTE 1: &lt;strong&gt;Docker&lt;/strong&gt; :  If getting maximum performance is not a requirement, I would suggest the reader to download Docker Images for Deep Learning packages freely available online, &lt;a href=&quot;https://hub.docker.com/r/kaixhin/cuda-theano/&quot;&gt;for e.g&lt;/a&gt;. Time required for all these standalone installation is justified only when these are going to be used over and again, for e.g in a research lab.&lt;/p&gt;

&lt;p&gt;NOTE 2: &lt;strong&gt;Redundancy&lt;/strong&gt; : Some of the commands can be combined to be done in a single line, especially installing libs from apt-get. But I prefer to run them one at a time, for more control and feedback on installation process.&lt;/p&gt;

&lt;p&gt;NOTE 3: &lt;strong&gt;Python 2&lt;/strong&gt; : While most of the scientific community is moving on with Python 3, it seems Deep Learning communmmunty is happy with Python 2. Thus all instllations except where mentioned pertain to Python 2.7.&lt;/p&gt;

&lt;p&gt;NOTE 4: &lt;strong&gt;Anaconda&lt;/strong&gt; : Why not Anaconda ? I have done some previous installations with Anaconda, but it becomes messy with smaller libs, and also compiling OpenCV. More importantly, I do not need all the utils of Anaconda, and on this machine I would like to minimize the libs installed.&lt;/p&gt;

&lt;h1 id=&quot;upgrade-the-ubuntu&quot;&gt;Upgrade the Ubuntu&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Always, upgrade the ubuntu for security and sanity purposes !&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get update        &lt;/code&gt;  # Fetches the list of available updates&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get upgrade       &lt;/code&gt;   # Strictly upgrades the current packages&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get dist-upgrade  &lt;/code&gt;   # Installs updates (new ones)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;raid-1&quot;&gt;RAID 1&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;## Partition and mount the harddrives
  Since my configuration has two HDD of 3 TB, I have configured them as RAID 1, this provides data redundancy. You do not want to lose weeks worth of training due to HDD crash ! If you do not want RAID 1, then skip this step.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y mdadm   &lt;/code&gt;   # Install mdadm, tool to manage RAID 1&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo mdadm --assemble --scan    &lt;/code&gt;   # check for existing raids ## found existing !! If not prepare, refer tutorial here &lt;a href=&quot;http://askubuntu.com/questions/526747/setting-up-raid-1-on-14-04-with-an-existing-drive&quot;&gt;http://askubuntu.com/questions/526747/setting-up-raid-1-on-14-04-with-an-existing-drive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mounting&quot;&gt;Mounting&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;df -aTh                         &lt;/code&gt;   # shows list of all mounts&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo mount /dev/md0 /media/hdd/ &lt;/code&gt;   # (Manual) mount existing&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;blkid                           &lt;/code&gt;   # shows uuid for drives to add to fstab
  Add the following line to /etc/fstab
# the RAID 1 mount of two hdd
UUID=06ad59d9-3176-4c16-95e9-77356cc572d7       /media/hdd      ext2    defaults    0    1&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo mount -a                   &lt;/code&gt;   # (Permanent) mount using fstab&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;install-essentials-extras-git-zsh&quot;&gt;Install Essentials, Extras, Git, Zsh&lt;/h1&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y build-essential&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y ubuntu-restricted-extras&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y vim&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y git&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y git-core&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;`sudo apt-get install -y checkinstall # allows for easy install/uninstall of packages from source&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y zsh&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y tmux&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y CMake&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y libopenblas-dev&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;`sudo apt-get install -y libhdf5-dev&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y gcc-4.8 &lt;/code&gt;   # because CUDA (7.0 &amp;amp; 7.5) works will less than 4.9.0
    &lt;ul&gt;
      &lt;li&gt;make soft link for gcc in /usr/bin&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y g++-4.8 &lt;/code&gt;   # because CUDA (7.0 &amp;amp; 7.5) works will less than 4.9.0
    &lt;ul&gt;
      &lt;li&gt;make soft link for g++ in /usr/bin&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y apache2&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;sudo /etc/init.d/apache2 start `   # start the Apache Server&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;python&quot;&gt;Python&lt;/h1&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;python-and-libs&quot;&gt;Python and Libs&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;python get-pip.py &lt;/code&gt;   # install pip&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install python-dev &lt;/code&gt;   # pythonLibs&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install libblas-dev liblapack-dev libatlas-base-dev gfortran&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip install cython git+https://github.com/scipy/scipy &lt;/code&gt;   # Installs Cython and Scipy both (Cython is requirement for scipy)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip install -U scikit-learn &lt;/code&gt;   # Requires numpy and scipy&lt;/li&gt;
  &lt;li&gt;Matplotlib better to install using APT, lot of system lib dependency
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y matplotlib&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;highend-computing&quot;&gt;Highend computing&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip install numpy&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip install markupsafe&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip install h5py&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip install nltk&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip install nose&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Install leveldb # for efficient use with Caffe, and probably other libs
    &lt;ul&gt;
      &lt;li&gt;git clone git@github.com:google/leveldb.git&lt;/li&gt;
      &lt;li&gt;make&lt;/li&gt;
      &lt;li&gt;sudo mv out-shared/libleveldb.* /usr/local/lib/&lt;/li&gt;
      &lt;li&gt;sudo cp -R include/leveldb /usr/local/include&lt;/li&gt;
      &lt;li&gt;sudo ldconfig&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;python3-and-ipython-jupyter&quot;&gt;Python3 and Ipython (Jupyter)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install python3-pip&lt;/code&gt;  # to install jupyter for python3, it needs pip3 and does not work using pip&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip install jupyter&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip3 install jupyter &lt;/code&gt;   # I don’t know why it requires sepearate installation, especially when not done using Anaconda !&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;nvidia&quot;&gt;NVIDIA&lt;/h1&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;install-nvidia-drivers&quot;&gt;Install NVIDIA Drivers&lt;/h2&gt;
&lt;p&gt;I would highly advice against installation using apt-get as it always installs one version older, and with NVIDIA every generation of driver brings performance boosts.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo service lightdm stop       &lt;/code&gt;   # stop the X server before running the installation
NOTE! Could not sign kernels, make a note of this. It might lead to problems with some libraries later on&lt;/li&gt;
  &lt;li&gt;Download AMD 64 bit Linux drivers from NVIDIA website (manual download and transfer and execute the provided script or binary)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;install-cuda&quot;&gt;Install CUDA&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ./cuda_7.0.28_linux.run --override  &lt;/code&gt;   # for cuda 7.0&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ln -s /usr/bin/g++-4.8 /usr/local/cuda/bin/g++ &lt;/code&gt;   # this might be omitted if a symbolic link has been made from /usr/bin/g++ like mentioned above&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ln -s /usr/bin/gcc-4.8 /usr/local/cuda/bin/gcc &lt;/code&gt;   # this might be omitted if a symbolic link has been made from /usr/bin/gcc like mentioned above&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;install-cudnn&quot;&gt;Install CuDNN&lt;/h2&gt;
&lt;p&gt;CuDNN improves the performance of deep learning libraries. Unfornately, not all of them work with latest version. Google’s Tensorflow requires v2, Theano &amp;amp; Caffe works with v3, whereas latest version is 4.
  To get the library you will have to register at &lt;a href=&quot;https://developer.nvidia.com/cudnn&quot;&gt;NVIDIA CuDNN&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Download the archive folder&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd &amp;lt;extracted_directory&amp;gt;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;export LD_LIBRARY_PATH=&lt;/code&gt;pwd&lt;code class=&quot;highlighter-rouge&quot;&gt;:$LD_LIBRARY_PATH&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although not required, I find it useful to rather install both the version of CuDNN (2 and 3). Put the libs of v2 in CUDA 6.5 or CUDA 7.0 (for Tensorflow), and v3 in CUDA 7.5.&lt;/p&gt;

&lt;h1 id=&quot;image-processing-and-computer-vision&quot;&gt;Image processing and Computer Vision&lt;/h1&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;install-opencv&quot;&gt;Install OpenCV&lt;/h2&gt;

&lt;p&gt;This is to install OpenCV 3.1 on Ubuntu 15.10. Thanks to the people at BVLC/Caffe, I did not have to tweak the settings, and pretty much their provided instructions worked.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get -y install libopencv-dev build-essential cmake git libgtk2.0-dev pkg-config python-dev python-numpy libdc1394-22 libdc1394-22-dev libjpeg-dev libpng12-dev libtiff5-dev libjasper-dev libavcodec-dev libavformat-dev libswscale-dev libxine2-dev libgstreamer0.10-dev libgstreamer-plugins-base0.10-dev libv4l-dev libtbb-dev libqt4-dev libfaac-dev libmp3lame-dev libopencore-amrnb-dev libopencore-amrwb-dev libtheora-dev libvorbis-dev libxvidcore-dev x264 v4l-utils unzip&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Download OpenCV from http://opencv.org/downloads.html and unpack. Enter the unpacked directory. Execute:&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mkdir build &amp;amp;&amp;amp; cd build/ &lt;/code&gt; # If you have Oh-my-zsh then just &lt;code class=&quot;highlighter-rouge&quot;&gt;take build&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=ON -D WITH_V4L=ON -D WITH_QT=ON -D WITH_OPENGL=ON ..&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;make -j12 # j 12 makes 12 parallel processes using all the cores to speed up the process, in a single core, the build might take couple of hours.&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo checkinstall # to create the deb package, and install it.&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;ffmpeg&quot;&gt;FFMPEG&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y ffmpeg&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;imagemagick&quot;&gt;ImageMagick&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y imagemagick&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;install-gpu-libraries&quot;&gt;Install GPU Libraries&lt;/h1&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;theano&quot;&gt;Theano&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip install Theano&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y python-pycuda &lt;/code&gt;   # also installs the dependencies, but it is best to have own installation of nvidia-cuda, to make sure the version is proper and to maintain multiple version installation&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tensorflow&quot;&gt;Tensorflow&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.6.0-cp27-none-linux_x86_64.whl&lt;/code&gt;
&amp;gt;  Tensorflow (as of Jan 15, 2016), works on Cuda 7.0 and CuDNN v2 ), thus change /usr/local/cuda softlink&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;keras&quot;&gt;Keras&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip install keras&lt;/code&gt;  # Could it get any easier !! Thanks people developer of Keras !&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lasagne&quot;&gt;Lasagne&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo pip install https://github.com/Lasagne/Lasagne/archive/master.zip&lt;/code&gt;  # Preferred way to install Lasagne !&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;torch&quot;&gt;Torch&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;curl -s https://raw.githubusercontent.com/torch/ezinstall/master/install-deps | bash&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;git clone https://github.com/torch/distro.git ~/torch --recursive&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd ~/torch; ./install.sh&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Aditionally to load caffe models in torch,
  * &lt;code class=&quot;highlighter-rouge&quot;&gt;luarocks install loadcaffe&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;caffe&quot;&gt;Caffe&lt;/h2&gt;

&lt;h3 id=&quot;note&quot;&gt;Note&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Switch back to GCC 5 and G++ 5, because the prebuilt libraries of Ubuntu 15.10 are built on gcc 5, and thus compiling caffe in 4.8 will
 not link them. However, caffe will refuse to compile with gcc 5, with error from following file
 &lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local/cuda/include/host_config.h&lt;/code&gt; Just comment out the line 115, which checks the version of GCC&lt;/li&gt;
  &lt;li&gt;Use the CMake instead of default make (checkout &lt;a href=&quot;https://github.com/BVLC/caffe/pull/1667&quot;&gt;https://github.com/BVLC/caffe/pull/1667&lt;/a&gt; )&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;requirements&quot;&gt;Requirements&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install --no-install-recommends libboost-all-dev&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;for req in $(cat python/requirements.txt); do pip install $req; done&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;issues&quot;&gt;Issues&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Linking Error in Leveldb e.g&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        Linking CXX executable caffe
        ../lib/libcaffe.so.1.0.0-rc3: undefined reference to `leveldb::Status::ToString[abi:cxx11]() const&#39;
        ../lib/libcaffe.so.1.0.0-rc3: undefined reference to `leveldb::DB::Open(leveldb::Options const&amp;amp;, std::__cxx11::basic_string&amp;lt;char, std::char_traits&amp;lt;char&amp;gt;, std::allocator&amp;lt;char&amp;gt; &amp;gt; const&amp;amp;, leveldb::DB**)&#39;
        collect2: error: ld returned 1 exit status
        tools/CMakeFiles/caffe.bin.dir/build.make:122: recipe for target &#39;tools/caffe&#39; failed
        make[2]: *** [tools/caffe] Error 1
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;

    &lt;p&gt;Solution, recompile Leveldb with GCC 5&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install libsnappy-dev&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wget https://leveldb.googlecode.com/files/leveldb-1.9.0.tar.gz\ntar -xzf leveldb-1.9.0.tar.gz\ncd leveldb-1.9.0make&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo mv libleveldb.* /usr/local/lib&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd include&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo cp -R leveldb /usr/local/include&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo ldconfiG&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Linking Error in Protobuf e.g&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        Linking CXX executable caffe
        CMakeFiles/caffe.bin.dir/caffe.cpp.o: In function `std::string* google::MakeCheckOpString&amp;lt;cudaError, cudaError&amp;gt;(cudaError const&amp;amp;, cudaError const&amp;amp;, char const*)&#39;:
        caffe.cpp:(.text._ZN6google17MakeCheckOpStringI9cudaErrorS1_EEPSsRKT_RKT0_PKc[_ZN6google17MakeCheckOpStringI9cudaErrorS1_EEPSsRKT_RKT0_PKc]+0x43): undefined reference to google::base::CheckOpMessageBuilder::NewString()&#39;
        CMakeFiles/caffe.bin.dir/caffe.cpp.o: In function `std::string* google::MakeCheckOpString&amp;lt;unsigned long, int&amp;gt;(unsigned long const&amp;amp;, int const&amp;amp;, char const*)&#39;:
        caffe.cpp:(.text._ZN6google17MakeCheckOpStringImiEEPSsRKT_RKT0_PKc[_ZN6google17MakeCheckOpStringImiEEPSsRKT_RKT0_PKc]+0x43): undefined reference to `google::base::CheckOpMessageBuilder::NewString()&#39;
        CMakeFiles/caffe.bin.dir/caffe.cpp.o: In function `main&#39;:
        caffe.cpp:(.text.startup+0x3e): undefined reference to `google::SetVersionString(std::string const&amp;amp;)&#39;
        caffe.cpp:(.text.startup+0x6e): undefined reference to `google::SetUsageMessage(std::string const&amp;amp;)&#39;
        ../lib/libcaffe.so.1.0.0-rc3: undefined reference to `google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(int, std::string const&amp;amp;, google::protobuf::io::CodedOutputStream*)
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;

    &lt;p&gt;Solution, change your current compiler to GCC
Use &lt;code class=&quot;highlighter-rouge&quot;&gt;gcc --version&lt;/code&gt; to make sure the correct version, and &lt;code class=&quot;highlighter-rouge&quot;&gt;which gcc&lt;/code&gt; to check the softlinks to actual gcc&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Relevant issues to checkout &lt;a href=&quot;https://github.com/BVLC/caffe/issues/2690&quot;&gt;https://github.com/BVLC/caffe/issues/2690&lt;/a&gt; and &lt;a href=&quot;https://github.com/BVLC/caffe/issues/3046&quot;&gt;https://github.com/BVLC/caffe/issues/3046&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In Ubuntu 15.10 when using GCC 5.2, compiling might fail as Protobuf was compiled with wrong flags. Add the following to Makefile
  &lt;code class=&quot;highlighter-rouge&quot;&gt;CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=0&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Issues with Google Lib Protobuf (even after the ABI CXX11 fix)&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd /usr/local/lib&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo mkdir libprotobuf_&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mv libprotobuf* libprotobuf_&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd ~&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;apt-get source libprotobuf-dev&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ll&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd protobuf-2.6.1&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;make&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;./configure&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;make -j12&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo make install&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;installation&quot;&gt;Installation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;mkdir build&lt;/li&gt;
  &lt;li&gt;cd build&lt;/li&gt;
  &lt;li&gt;cmake .. # &lt;em&gt;Make kept giving me issues, but CMake worked.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;make all&lt;/li&gt;
  &lt;li&gt;make test&lt;/li&gt;
  &lt;li&gt;make runtest&lt;/li&gt;
  &lt;li&gt;make pycaffe # &lt;em&gt;To be able to import caffe&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;to-be-updated-with-instructions-for-nervana&quot;&gt;To be updated with instructions for Nervana&lt;/h2&gt;

&lt;h1 id=&quot;optional&quot;&gt;Optional&lt;/h1&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;remove-unnecessary-ubuntu-folders&quot;&gt;Remove unnecessary Ubuntu folders&lt;/h2&gt;
&lt;p&gt;When working with only terminal via ssh, you do not need some of these folders //&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rm -rf ~/Desktop&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rm -rf ~/Public &lt;/code&gt; # Is not persistent, after restart Ubuntu recreates this directory&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rm -rf ~/Pictures&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rm -rf ~/Music&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rm -rf ~/Videos&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rm -rf ~/Downloads&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rm -rf ~/Templates&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rm -rf ~/Documents&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rm -rf ~/examples.desktop&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;customization&quot;&gt;Customization&lt;/h2&gt;
&lt;p&gt;These are my personal customization which I do on every machine I use. You may find them useful. They increase visual appeal and efficiency of working on Linux Machine.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;chsh -s which zsh&lt;/code&gt;              # Change the shell to ZSH (which zsh should be inside backticks, but Github Markdown messes up the formatting)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo reboot now                 &lt;/code&gt;   # Requires restart&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;git clone git@github.com:iamaaditya/dotfiles.git&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~/dotfiles/install.sh           &lt;/code&gt;   # Run the commands to make the shortcuts&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim &lt;/code&gt;   # Install bundle
### Solarized Color for vim&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd ~/.vim/colors/ &lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wget https://raw.githubusercontent.com/altercation/vim-colors-solarized/master/colors/solarized.vim&lt;/code&gt;
### Powerline &amp;amp; associated fonts&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pip install powerline-status    &lt;/code&gt;   # Powerline&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wget https://github.com/Lokaltog/powerline/raw/develop/font/PowerlineSymbols.otf https://github.com/Lokaltog/powerline/raw/develop/font/10-powerline-symbols.conf&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mkdir -p ~/.fonts/ &amp;amp;&amp;amp; mv PowerlineSymbols.otf ~/.fonts/&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fc-cache -vf ~/.fonts&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mkdir -p ~/.config/fontconfig/conf.d/ &amp;amp;&amp;amp; mv 10-powerline-symbols.conf ~/.config/fontconfig/conf.d/&lt;/code&gt;
Note - if some aspects of powerline does not show up, check &lt;a href=&quot;http://askubuntu.com/questions/283908/how-can-i-install-and-use-powerline-plugin&quot;&gt;http://askubuntu.com/questions/283908/how-can-i-install-and-use-powerline-plugin&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are curious on what this customization looks like, take a look at this screenshot. Here it shows Tmux session with multiple windows, and a multiple panes in the same window. Switching windows and panes in Tmux is very easy. Copy information across windows, sessions and servers is really at the tip of the finger. Also Solarized colors make it tolerable to use the screen for hours.
  &lt;img src=&quot;https://raw.githubusercontent.com/iamaaditya/iamaaditya.github.io/master/images/Screenshot.png&quot; alt=&quot;screenshot&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 15 Jan 2016 00:00:00 -0500</pubDate>
        <link>http://iamaaditya.github.io/2016/01/Deep-Learning-software-installation-guide-on-fresh-Ubuntu/</link>
        <guid isPermaLink="true">http://iamaaditya.github.io/2016/01/Deep-Learning-software-installation-guide-on-fresh-Ubuntu/</guid>
      </item>
    
      <item>
        <title>Extension of Hidden Markov Models &amp;#8211; Formulas in two state HMM</title>
        <description>&lt;p&gt;Extending the hidden markov models, where the current state is affected by past two states. This can be useful in simulation of games or in less sophisticated pricing models (more accurate would be an exponential decay of all past terms).&lt;/p&gt;

&lt;h2 id=&quot;notations&quot;&gt;Notations&lt;/h2&gt;

&lt;p&gt;I have kept the notations close to as provided in Chapter 6 of “Speech and Language Processing”, Second Edition by Martin and Jurafsky.&lt;/p&gt;

&lt;p&gt;\(\lambda\) Common term for all HMM parameters. All the probabilties&lt;br /&gt;
will be conditioned to this term i.e \(P(……|\lambda)\)&lt;/p&gt;

&lt;p&gt;\(T\) denotes the total number of time steps&lt;/p&gt;

&lt;p&gt;\(N\) denotes the total number of states&lt;/p&gt;

&lt;p&gt;\(o_{t}\) denotes the observed variable (state) at time step t&lt;/p&gt;

&lt;p&gt;\(q_{t}\) denotes the hidden variale (state) at time step t&lt;/p&gt;

&lt;h2 id=&quot;formulas&quot;&gt;Formulas&lt;/h2&gt;

&lt;h3 id=&quot;distribution-of-alphatij&quot;&gt;Distribution of \(\alpha_{t}(i,j)\)&lt;/h3&gt;

&lt;p&gt;\(\alpha_{t}(i,j)\) denotes the joint probability distribution of all observed variables until time \(t\) and current and the last states.&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
\alpha_{t}(i,j)=P(o_{1},o_{2},\dots,o_{t},q_{t-1}=i,q_{t}=j|\lambda)&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;Where \(\lambda\) is the given HMM parameters.&lt;/p&gt;

&lt;h3 id=&quot;base-case-of-alphatij&quot;&gt;Base case of \(\alpha_{t}(i,j)\)&lt;/h3&gt;

&lt;p&gt;Consider the following notation:&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
a_{ij}=P(q_{t}=j|q_{t-1}=i)&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
a_{ijk}=P(q_{t+1}=k|q_{t}=j,q_{t-1}=i)&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;then base cases are –&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
\alpha_{1}(i=0,j)=a_{i=0,j}b_{j}(o_{1})&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
\alpha_{2}(i,j)=\alpha_{1}(0,j)a_{ij}b_{j}(o_{2})&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;It should be noted that if the second state (\(q_{2}\() is also allowed to be entry point, then it needs an additional base case of —&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
\alpha_{2}(i=0,j)=\sum_{i=1}^{N}\alpha_{1}(i=0,j)a_{ij}b_{j}(o_{2})&lt;br /&gt;
\]&lt;/p&gt;

&lt;h3 id=&quot;inductive-step&quot;&gt;Inductive step&lt;/h3&gt;

&lt;p&gt;\[&lt;br /&gt;
\alpha_{t}(j,k)=\sum_{i=1}^{N}\alpha_{t-1}(i,j)\times a_{ijk}\times b_{k}(o_{t})\text{ where, }3\leq t\leq T&lt;br /&gt;
\]&lt;/p&gt;

&lt;h3 id=&quot;termination-step&quot;&gt;Termination step&lt;/h3&gt;

&lt;p&gt;\[&lt;br /&gt;
P(O|\lambda)=\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_{T}(i,j)\times a_{iF}\times a_{jF}\times a_{ijF}&lt;br /&gt;
\]&lt;/p&gt;

&lt;h2 id=&quot;estimated-expected-transitions&quot;&gt;Estimated Expected Transitions&lt;/h2&gt;

&lt;h3 id=&quot;xit&quot;&gt;\(\xi_{t}\)&lt;/h3&gt;

&lt;p&gt;\(\xi_{t}\) = Probability of being in state k at t+1, j at t and i at t-1&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
\xi_{t}=P(q_{t+1}=k,q_{t}=j,q_{t-1}=i|o_{1},\dots o_{T})=\frac{\alpha_{t}(i,j)\times\beta_{t+1}(j,k)\times a_{ijk}b_{k}(o_{t+1})}{\alpha(q_{f})}&lt;br /&gt;
\]&lt;/p&gt;

&lt;h3 id=&quot;gammat&quot;&gt;\(\gamma_{t}\)&lt;/h3&gt;

&lt;p&gt;\(\gamma_{t}\) = Probability of being in state i at ‘t’, given all the obervations&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
\gamma_{t}=P(q_{t}=i|o_{1},\dots,o_{T})=\frac{\sum_{j}^{N}\alpha(j,i),\beta(j,i)}{\alpha(q_{f})}&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;where \(\alpha(q_{f})=P(o_{1},\dots,o_{T})\) (joint probability of all observed variables)&lt;/p&gt;

&lt;p&gt;{*}Please note that for \(\gamma_{t}\) values, the \(i\) and \(j\) are interchanged in \(\alpha\) and \(\beta\) values, because I wanted to keep the input as \(q_{t}=i\), as provided in the problem, whereas convention in textbook is to denote \(i\) as antecedent to \(j\).&lt;/p&gt;

&lt;p&gt;&lt;a title=&quot;Proofs for the two state hidden markov model&quot; href=&quot;http://aaditya.info/research/hmm_two_states.pdf&quot; target=&quot;_blank&quot;&gt;Detailed proofs has been provided here&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 10 Feb 2015 00:00:00 -0500</pubDate>
        <link>http://iamaaditya.github.io/2015/02/extension-of-hidden-markov-models-formulas-in-two-state-hmm/</link>
        <guid isPermaLink="true">http://iamaaditya.github.io/2015/02/extension-of-hidden-markov-models-formulas-in-two-state-hmm/</guid>
      </item>
    
      <item>
        <title>Pseudo loss function in distributed Adaboost</title>
        <description>&lt;h1 id=&quot;lhedgebeta&quot;&gt;&lt;script type=&quot;math/tex&quot;&gt;L_{Hedge(\beta)}&lt;/script&gt;&lt;/h1&gt;

&lt;p&gt;This is the sketch of proof of correctness of &lt;img src=&quot;http://s0.wp.com/latex.php?latex=L_%7BHedge%28%5Cbeta%29%7D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;L_{Hedge(\beta)} &quot; title=&quot;L_{Hedge(\beta)} &quot; class=&quot;latex&quot; /&gt;, pseudo loss function, in the case of distributed boosting algorithm. It has been known that boosting of functions with each applicator with more than 0.5 accuracy is sufficient to guarantee lowest minimum accuracy in iterative pooling. A similar sketch is presented for the distributed case where the score from each stage is not shared across all the computing nodes. While this guarantees the correctness, it does not guarantee the convergence, which is still a highly sought after problem in distributed algorithm.&lt;/p&gt;

&lt;p&gt;For people uninitiated in the problem, it is highly advised to &lt;a href=&quot;http://www-users.cs.umn.edu/~aleks/papers/kdd_01.pdf&quot;&gt;read this paper&lt;/a&gt; or for much simpler and faster read, &lt;a href=&quot;http://www-users.cs.umn.edu/~banerjee/Teaching/Spring06/talks/Paper02Tim1.ppt&quot;&gt;these slides&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;notations&quot;&gt;Notations&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;http://s0.wp.com/latex.php?latex=w+%3D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;w = &quot; title=&quot;w = &quot; class=&quot;latex&quot; /&gt; weights Vector&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://s0.wp.com/latex.php?latex=T+%3D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;T = &quot; title=&quot;T = &quot; class=&quot;latex&quot; /&gt; Number of iterations&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://s0.wp.com/latex.php?latex=%5Cbeta&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;\beta&quot; title=&quot;\beta&quot; class=&quot;latex&quot; /&gt; = Parameter &lt;img src=&quot;http://s0.wp.com/latex.php?latex=%5Cepsilon%5B0%2C1%5D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;\epsilon[0,1] &quot; title=&quot;\epsilon[0,1] &quot; class=&quot;latex&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://s0.wp.com/latex.php?latex=%5Cell%5E%7Bt%7D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;\ell^{t} &quot; title=&quot;\ell^{t} &quot; class=&quot;latex&quot; /&gt;= Loss Vector for the &lt;img src=&quot;http://s0.wp.com/latex.php?latex=t+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;t &quot; title=&quot;t &quot; class=&quot;latex&quot; /&gt; iteration (trials)&lt;/p&gt;

&lt;p&gt;N = Total Number of weak learners&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://s0.wp.com/latex.php?latex=%7BL_i%3D+%7D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;{L_i= } &quot; title=&quot;{L_i= } &quot; class=&quot;latex&quot; /&gt;Strategy i’s cumulative loss = &lt;img src=&quot;http://s0.wp.com/latex.php?latex=%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Cell_%7Bi%7D%5E%7Bt%7D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;\sum_{t=1}^{T}\ell_{i}^{t} &quot; title=&quot;\sum_{t=1}^{T}\ell_{i}^{t} &quot; class=&quot;latex&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://s0.wp.com/latex.php?latex=L_%7BA%7D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;L_{A} &quot; title=&quot;L_{A} &quot; class=&quot;latex&quot; /&gt;= Algorithm A’s total cumulative loss = &lt;img src=&quot;http://s0.wp.com/latex.php?latex=%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Cmathbf%7Bp%7D%5E%7Bt%7D.%5Cell%5E%7Bt%7D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;\sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t} &quot; title=&quot;\sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t} &quot; class=&quot;latex&quot; /&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;h2 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h2&gt;

&lt;p&gt;Consider the following Algorithm&lt;/p&gt;

&lt;p&gt;Hedge(&lt;img src=&quot;http://s0.wp.com/latex.php?latex=%5Cbeta+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;\beta &quot; title=&quot;\beta &quot; class=&quot;latex&quot; /&gt;) :&lt;/p&gt;

&lt;p&gt;Do for t = 1, 2, …, &lt;em&gt;T&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Choose Allocation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;\[&lt;br /&gt;
\mathbf{p}^{t}=\frac{\mathbf{w}^{t}}{\sum_{i-1}^{N}w_{i}^{t}}&lt;br /&gt;
\]&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Receive loss vector &lt;img src=&quot;http://s0.wp.com/latex.php?latex=%5Cell%5E%7Bt%7D%5Cepsilon%5B0%2C1%5D%5E%7BN%7D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;\ell^{t}\epsilon[0,1]^{N} &quot; title=&quot;\ell^{t}\epsilon[0,1]^{N} &quot; class=&quot;latex&quot; /&gt; from environment&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Suffer loss &lt;img src=&quot;http://s0.wp.com/latex.php?latex=%5Cmathbf%7Bp%7D%5E%7Bt%7D.%5Cell%5E%7Bt%7D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;\mathbf{p}^{t}.\ell^{t} &quot; title=&quot;\mathbf{p}^{t}.\ell^{t} &quot; class=&quot;latex&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Set the new weights&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;\[&lt;br /&gt;
w_{i}^{t+1}=w_{i}^{t}\beta^{\ell_{i}^{t}}&lt;br /&gt;
\]&lt;/p&gt;

&lt;h2 id=&quot;to-prove&quot;&gt;To Prove&lt;/h2&gt;

&lt;p&gt;\[&lt;br /&gt;
ln(\sum_{i=1}^{N}w_{i}^{T+1})\leq-(1-\beta)L_{Hedge(\beta)}&lt;br /&gt;
\]&lt;/p&gt;

&lt;h2 id=&quot;proof&quot;&gt;Proof&lt;/h2&gt;

&lt;p&gt;From the Hedge(&lt;img src=&quot;http://s0.wp.com/latex.php?latex=%5Cbeta+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;\beta &quot; title=&quot;\beta &quot; class=&quot;latex&quot; /&gt;) function, we know that&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
\sum_{i=1}^{N}w_{i}^{t+1}=\sum_{i=1}^{N}w_{i}^{t}\beta^{\ell_{i}^{t}}&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;For &lt;img src=&quot;http://s0.wp.com/latex.php?latex=%5Calpha%5Cgeq0+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;\alpha\geq0 &quot; title=&quot;\alpha\geq0 &quot; class=&quot;latex&quot; /&gt; and &lt;img src=&quot;http://s0.wp.com/latex.php?latex=r%5Cepsilon%5B0%2C1%5D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;r\epsilon[0,1] &quot; title=&quot;r\epsilon[0,1] &quot; class=&quot;latex&quot; /&gt;, by convexity we know that&lt;br /&gt;
&lt;img src=&quot;http://s0.wp.com/latex.php?latex=%5Calpha%5E%7Br%7D%5Cleq1-%281-%5Calpha%29r&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;\alpha^{r}\leq1-(1-\alpha)r&quot; title=&quot;\alpha^{r}\leq1-(1-\alpha)r&quot; class=&quot;latex&quot; /&gt;, therefore above eqn can be rewritten&lt;br /&gt;
as,&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
\sum_{i=1}^{N}w_{i}^{t+1}\leq\sum_{i=1}^{N}w_{i}^{t}(1-(1-\beta)\ell_{i}^{t})&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
\sum_{i=1}^{N}w_{i}^{t+1}\leq\left(\sum_{i=1}^{N}w_{i}^{t}\right)(1-(1-\beta)\mathbf{p}^{t}.\ell_{i}^{t})&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;For all the trials &lt;img src=&quot;http://s0.wp.com/latex.php?latex=t%3D1%2C%5Cldots%2CT+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;t=1,\ldots,T &quot; title=&quot;t=1,\ldots,T &quot; class=&quot;latex&quot; /&gt;&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
\sum_{i=1}^{N}w_{i}^{t+1}\leq\prod_{t=1}^{T}(1-(1-\beta)\mathbf{p}^{t}.\ell^{t}&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;(since &lt;img src=&quot;http://s0.wp.com/latex.php?latex=1%2Bx%5Cleq+e%5E%7Bx%7D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;1+x\leq e^{x} &quot; title=&quot;1+x\leq e^{x} &quot; class=&quot;latex&quot; /&gt; )&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
\sum_{i=1}^{N}w_{i}^{t+1}\leq exp\left(-(1-\beta)\sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t}\right)&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;Taking log on both sides&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
ln\left(\sum_{i=1}^{N}w_{i}^{t+1}\right)\leq-(1-\beta)\sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t}&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;Taking negative sign and switching the equality&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
(1-\beta)\sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t}\leq ln\left(\sum_{i=1}^{N}w_{i}^{t+1}\right)&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
\sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t}\leq\frac{ln\left(\sum_{i=1}^{N}w_{i}^{t+1}\right)}{(1-\beta)}&lt;br /&gt;
\]&lt;/p&gt;

&lt;p&gt;Now from the definition of &lt;img src=&quot;http://s0.wp.com/latex.php?latex=L_%7BA%7D+%3D+%5Csum_%7Bt%3D1%7D%5E%7BT%7D%5Cmathbf%7Bp%7D%5E%7Bt%7D.%5Cell%5E%7Bt%7D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;L_{A} = \sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t} &quot; title=&quot;L_{A} = \sum_{t=1}^{T}\mathbf{p}^{t}.\ell^{t} &quot; class=&quot;latex&quot; /&gt;,&lt;br /&gt;
since the algorithm here is Hedge(&lt;img src=&quot;http://s0.wp.com/latex.php?latex=%5Cbeta+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;\beta &quot; title=&quot;\beta &quot; class=&quot;latex&quot; /&gt;), where &lt;img src=&quot;http://s0.wp.com/latex.php?latex=%5Cbeta+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;\beta &quot; title=&quot;\beta &quot; class=&quot;latex&quot; /&gt;is the&lt;br /&gt;
parameter, &lt;img src=&quot;http://s0.wp.com/latex.php?latex=L_%7BA%7D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;L_{A} &quot; title=&quot;L_{A} &quot; class=&quot;latex&quot; /&gt;, is called as &lt;img src=&quot;http://s0.wp.com/latex.php?latex=L_%7BHedge%28%5Cbeta%29%7D+&amp;#038;bg=ffffff&amp;#038;fg=000000&amp;#038;s=0&quot; alt=&quot;L_{Hedge(\beta)} &quot; title=&quot;L_{Hedge(\beta)} &quot; class=&quot;latex&quot; /&gt;&lt;/p&gt;

&lt;p&gt;therefore,&lt;/p&gt;

&lt;p&gt;\[&lt;br /&gt;
L_{Hedge(\beta)}\leq\frac{ln\left(\sum_{i=1}^{N}w_{i}^{t+1}\right)}{(1-\beta)}&lt;br /&gt;
\]&lt;/p&gt;

&lt;p style=&quot;text-align: right;&quot;&gt;
  &lt;span style=&quot;text-decoration: underline;&quot;&gt;QED&lt;/span&gt;
&lt;/p&gt;

</description>
        <pubDate>Wed, 10 Dec 2014 00:00:00 -0500</pubDate>
        <link>http://iamaaditya.github.io/2014/12/pseudo-loss-function-in-distributed-adaboost/</link>
        <guid isPermaLink="true">http://iamaaditya.github.io/2014/12/pseudo-loss-function-in-distributed-adaboost/</guid>
      </item>
    
      <item>
        <title>Dokuwiki for Personal Notebook and Note taking</title>
        <description>&lt;p&gt;As a researcher, you soon start wondering if you had centralized all your notes, possibly digitized them, life would be much better. Recently when I had to make a tough choice of leaving all my notes from years when I am about to shift country &lt;em&gt;(due to limited air travel baggage)&lt;/em&gt;, I wish I had them on computer. Since I will be making more notes in the future, at least a lesson is learnt.&lt;/p&gt;

&lt;p&gt;For this reason, I started experimenting with different available tools. I tried Evernote, Catch.com, Microsoft Onenote and various personal wikis. Initially, I liked MS Onenote. Since it has several useful tools and very easy to use ‘capturing methods’. Later, I realized having it online would be more beneficial as I will be able to access it from anywhere and perhaps share it with friends/colleague if needed.&lt;/p&gt;

&lt;p&gt;Then I started experimenting with several ‘wiki’ sites. I tried ‘&lt;a href=&quot;http://tiddlywiki.com/&quot; target=&quot;_blank&quot;&gt;tiddlywiki&lt;/a&gt;‘ – works great, very fluid, no installation, single file. But the idea that all the notes are in single file made me uncomfortable. I realized very soon these notes will be running thousands of pages and all of them in a single file will cause it to load very slowly. Also to have it online, I will have to give higher privileges to the file &lt;em&gt;(so that it can modify the source itself)&lt;/em&gt;, which I didn’t like. ‘Mediawiki’ (from Wikipedia.org) looked little overkill.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comparision of various personal wiki services/softwares&lt;/strong&gt; is given &lt;a href=&quot;http://www.wikimatrix.org/compare/DokuWiki+MediaWiki+bLADE-Wiki+6of5-Wiki&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;http://en.wikipedia.org/wiki/Comparison_of_wiki_software&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Long story short, I ended up using &lt;strong&gt;&lt;a href=&quot;https://www.dokuwiki.org&quot; target=&quot;_blank&quot;&gt;Dokuwiki&lt;/a&gt;&lt;/strong&gt;. Great &lt;a href=&quot;https://www.dokuwiki.org/features&quot; target=&quot;_blank&quot;&gt;features&lt;/a&gt;, many &lt;a href=&quot;https://www.dokuwiki.org/features&quot; target=&quot;_blank&quot;&gt;plugins &lt;/a&gt;and with little knowledge of php you can customize it with ease. Last but not least, great support for $ \LaTeX $&lt;/p&gt;

&lt;p&gt;You can see my installation &lt;em&gt;(link given below)&lt;/em&gt;, although the Notes are still in nascent stage, but I soon expect it to be full of information and my personal notes, especially in Maths and Computer Science.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://aaditya.info/wiki&quot; title=&quot;Personal Notebook using Dokuwiki&quot; target=&quot;_blank&quot;&gt;My Personal Notebook using Dokuwiki&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This also means, I will not be using this blog anymore for personal notes. My initial notes and rough drafts will be in wiki and final and polished articles will be in this blog.&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Mar 2013 00:00:00 -0500</pubDate>
        <link>http://iamaaditya.github.io/2013/03/dokuwiki-for-personal-notebook-and-note-taking/</link>
        <guid isPermaLink="true">http://iamaaditya.github.io/2013/03/dokuwiki-for-personal-notebook-and-note-taking/</guid>
      </item>
    
      <item>
        <title>Solutions to Hamilton-Jacobi-Bellman under uncertainity</title>
        <description>&lt;p&gt;After doing some reading on decision under un-certainity, I get the feeling that this I will be looking more into this. More so because I have the feeling like there is more to this field, lot of unknowns yet(which is still partly due to my lack of profound knowledge in the field). I feel this field is yet to mature.&lt;/p&gt;

&lt;p&gt;Solution to Hamilton-Jacobi-Bellman (great story on how the -Bellman part was added to the equation) has been worked by several researchers, but I am looking into the prospect of applying the same under ‘uncertainity’.&lt;/p&gt;

&lt;p&gt;The problem:&lt;br /&gt;
\[   V(x(0), 0) = \min_u  \int_0^T C[x(t),u(t)]\,dt + D[x(T)] \]&lt;/p&gt;

&lt;p&gt;The Solution:&lt;br /&gt;
\[  \dot{V}(x,t) + \min_u ( \nabla V(x,t) \cdot F(x, u) + C(x,u) ) = 0 \]&lt;/p&gt;

&lt;p&gt;usual constraints and conditions apply, full description at &lt;a title=&quot;Hamilton-Jacobi-Equation&quot; href=&quot;http://en.wikipedia.org/wiki/Hamilton%E2%80%93Jacobi%E2%80%93Bellman_equation&quot; target=&quot;_blank&quot;&gt;this wiki page&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Last one month, I did some work; tried my hand with varies forms of solution. I looked into Stochastic Approaches in CFD simulations (Stochastic collocation), Chebyshev Polynomials, Galerkin Approximation, Pontryagin Maximum Principle). But I have failed and right now in life other priorities (grad-school applications) is keeping me from giving another serious look. (This work was part of the reason why no blog posts for a whole month).&lt;/p&gt;

&lt;p&gt;I want to come back to this topic and work more, in the meantime, I am making note of some of the papers and materials I referred and will be doing so again in the near future.&lt;/p&gt;

&lt;h3 id=&quot;light-introduction-and-derivation-of-hamilton-jacobi-bellman&quot;&gt;Light introduction and derivation of Hamilton-Jacobi-Bellman&lt;/h3&gt;

&lt;p&gt;Ian Mitchell&lt;/p&gt;

&lt;p&gt;http://www.cs.ubc.ca/~mitchell/Class/CS532M.2007W2/Talks/hjbSham.pdf&lt;/p&gt;

&lt;p&gt;&lt;em&gt;general read&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;stochastic-perron8217s-method-for-hamilton-jacobi-bellman-equations&quot;&gt;Stochastic Perron’s method for Hamilton-Jacobi-Bellman equations&lt;/h3&gt;

&lt;p&gt;Bayraktar et al&lt;/p&gt;

&lt;p&gt;http://arxiv.org/abs/1212.2170&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;new approach *&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;hamilton-jacobi-bellman-equations-8211-analysis-and-numerical-analysis&quot;&gt;Hamilton-Jacobi-Bellman Equations – Analysis and Numerical Analysis&lt;/h3&gt;

&lt;p&gt;Iain Smears&lt;/p&gt;

&lt;p&gt;http://www.maths.dur.ac.uk/Ug/projects/highlights/PR4/Smears_HJB_report.pdf&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Complete Thesis *&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;stochastic-approaches-to-uncertainity-quantification-in-cfd-simulations&quot;&gt;Stochastic Approaches To Uncertainity Quantification In CFD Simulations&lt;/h3&gt;

&lt;p&gt;Mathelin et al&lt;/p&gt;

&lt;p&gt;http://perso.limsi.fr/Individu/mathelin/research/NumUNC_MHZ.pdf&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Inspiration from CFD *&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;solution-of-hamilton-jacobi-bellman-equations&quot;&gt;Solution of Hamilton Jacobi Bellman Equations&lt;/h3&gt;

&lt;p&gt;Navasca et al&lt;/p&gt;

&lt;p&gt;http://people.clarkson.edu/~cnavasca/KN1.pdf&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;using Pontryagin maximum principle *&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;potential-use-of-ito-lemma&quot;&gt;Potential use of Ito lemma&lt;/h3&gt;

&lt;p&gt;See &lt;a title=&quot;Ito&#39;s Lemma&quot; href=&quot;http://en.wikipedia.org/wiki/It%C5%8D_calculus#It.C5.8D.27s_lemma&quot; target=&quot;_blank&quot;&gt;this equation&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Mar 2013 00:00:00 -0500</pubDate>
        <link>http://iamaaditya.github.io/2013/03/solutions-to-hamilton-jacobi-bellman-under-uncertainity/</link>
        <guid isPermaLink="true">http://iamaaditya.github.io/2013/03/solutions-to-hamilton-jacobi-bellman-under-uncertainity/</guid>
      </item>
    
      <item>
        <title>Moments and transition probability of Trinomial Tree</title>
        <description>&lt;p&gt;I just switched from using WP-Latex to MathJax since Chrome is soon going to have built-in support for MathJax and it is easier to recover the tex codes for the reader.&lt;/p&gt;

&lt;p&gt;To test out the functionality, I wanted to write down some equations befitting to the occasion. I scraped through my notes to see if there was anything interesting and worth posting. I stumbled upon &lt;a href=&quot;http://en.wikipedia.org/wiki/Trinomial_tree&quot; title=&quot;Trinomial Tree Wiki&quot; target=&quot;_blank&quot;&gt;Trinomial Tree&lt;/a&gt; ( \ddagger ). Now it is not that common and haven’t seen much usage of Trinomial Tree outside finance (One of the ways for Options Pricing), nonetheless I thought it would be interesting to refresh these formulas, you never know when you might need them again.&lt;/p&gt;

&lt;p&gt;Trinomial Tree, a special case of binomial tree where instead of two we have three branches, lets call them Up ( u ) , Middle ( m ) and Down (d ) . Transition of each step could then be defined as —&lt;/p&gt;

&lt;p&gt;  \( S(t + \Delta t) = S(t) \times u \) with probability  \( p_u \)&lt;/p&gt;

&lt;p&gt;  \( S(t + \Delta t) = S(t) \times 1 \) with probability  \( 1 - p_u - p_d \)&lt;/p&gt;

&lt;p&gt;  \( S(t + \Delta t) = S(t) \times d \) with probability  \( p_d \)&lt;/p&gt;

&lt;p&gt;For simplicity sake let’s consider that transition magnitude is same on each side, i.e&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;u = e^{\sigma \sqrt{2 \Delta t} } \, , \,\, d = e^{-\sigma \sqrt{2 \Delta t }} \, , \,\, m=1&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;then the &lt;strong&gt;transition probability&lt;/strong&gt; is given by&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
p_u = \left ( \frac{e^\frac{r \Delta t}{2} &amp;#8211; e^{-\sigma\sqrt{\frac{r \Delta t}{2}}}}{e^{\sigma\sqrt{\frac{r \Delta t}{2}}} &amp;#8211; e^{-\sigma\sqrt{\frac{r \Delta t}{2}}}} \right ) ^2 %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
p_u = \left ( \frac{ e^{\sigma\sqrt{\frac{r \Delta t}{2}}} + e^\frac{r \Delta t}{2} }{e^{\sigma\sqrt{\frac{r \Delta t}{2}}} &amp;#8211; e^{-\sigma\sqrt{\frac{r \Delta t}{2}}}} \right ) ^2 %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
p\_m = 1 &amp;#8211; p\_u &amp;#8211; p_d %]]&gt;&lt;/script&gt;

&lt;p&gt;from above equations we can derive the **moments **as given below,&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\mathbb{E}[S(t\_{i+1})|S(t\_i)] = e^{r \Delta t}S(t_i)&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{V}ar[S(t\_{i+1})|S(t\_i)] = \Delta t S(t_i)^2 \sigma ^2 + \mathcal{O}(\Delta t^\frac{3}{2})&lt;/script&gt;

&lt;p&gt;\( \ddagger \) &lt;a href=&quot;http://en.wikipedia.org/wiki/Phelim_Boyle&quot; title=&quot;Phelim Boyle&quot; target=&quot;_blank&quot;&gt;Phelim Boyle&lt;/a&gt; in 1986.&lt;/p&gt;

&lt;p&gt;An excellent&lt;a href=&quot;http://www41.homepage.villanova.edu/klaus.volpert/teaching/financial_math/Fall10/Presentations/Trinomial.pptx&quot; title=&quot;Background on Trinomial Tree&quot; target=&quot;_blank&quot;&gt; 23 slide &lt;/a&gt;background on Trinomial including other forms besides Boyle’s.&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Jan 2013 00:00:00 -0500</pubDate>
        <link>http://iamaaditya.github.io/2013/01/moments-and-transition-probability-of-trinomial-tree/</link>
        <guid isPermaLink="true">http://iamaaditya.github.io/2013/01/moments-and-transition-probability-of-trinomial-tree/</guid>
      </item>
    
      <item>
        <title>Pascal&amp;#8217;s Triangle in Standard ML</title>
        <description>&lt;p&gt;It has been a while that I posted something (grad school applications !). For past few weeks I have been learning Standard ML (SML), my first foray into functional programming language. I must say, I was skeptical at first due to ‘no-state’ concept but it is turning out to be great experience. Recursion can only be appreciated when you have to write programs without loop. This makes me rethink about learning Scala and Haskell.&lt;/p&gt;

&lt;p&gt;For a nice 73 slide introduction on SML &lt;a href=&quot;http://courses.cs.vt.edu/~cs3304/Spring02/lectures/lect04.pdf&quot; title=&quot;Introduction to SML&quot; target=&quot;_blank&quot;&gt;see this&lt;/a&gt;. One of power of SML lies in doing proofs. For those who are already intiated with SML, you might want to look &lt;a href=&quot;https://github.com/agentcoops/SML-Proof-Manipulation&quot; title=&quot;SML Proof Manipulation&quot; target=&quot;_blank&quot;&gt;SML Proof Manipulation&lt;/a&gt;. If you want to learn how to start writing proofs in SML then here is a nice tutorial on the same &lt;a href=&quot;http://www.cs.bham.ac.uk/research/projects/poplog/paradigms_lectures/Theorem.html&quot; title=&quot;Proofs in SML&quot;&gt;Implementing Constructive Proof Rules for SML in MetaSML&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I like to write Pascal’s triangle as my generic hello world program. Given below is my attempt at pascal’s triangle, a beautiful binomial expansion tree. Though I will confess that this is not the most elegant solution to Pascal’s triangle but I have considered readibility over conciseness.&lt;/p&gt;

&lt;div class=&quot;wp_syntax&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td class=&quot;line_numbers&quot;&gt;
        &lt;pre&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;
      &lt;/td&gt;
      
      &lt;td class=&quot;code&quot;&gt;
        &lt;pre class=&quot;pascal&quot; style=&quot;font-family:monospace;&quot;&gt;fun choose&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#40;&lt;/span&gt;r &lt;span style=&quot;color: #000066;&quot;&gt;:&lt;/span&gt; int&lt;span style=&quot;color: #000066;&quot;&gt;,&lt;/span&gt; k &lt;span style=&quot;color: #000066;&quot;&gt;:&lt;/span&gt; int&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#41;&lt;/span&gt; &lt;span style=&quot;color: #000066;&quot;&gt;=&lt;/span&gt; 
    &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;if&lt;/span&gt; k &lt;span style=&quot;color: #000066;&quot;&gt;=&lt;/span&gt; &lt;span style=&quot;color: #cc66cc;&quot;&gt;1&lt;/span&gt; &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;then&lt;/span&gt; r
    &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;else&lt;/span&gt; &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;if&lt;/span&gt; k &lt;span style=&quot;color: #000066;&quot;&gt;=&lt;/span&gt; &lt;span style=&quot;color: #cc66cc;&quot;&gt;&lt;/span&gt; orelse k &lt;span style=&quot;color: #000066;&quot;&gt;=&lt;/span&gt; r &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;then&lt;/span&gt; &lt;span style=&quot;color: #cc66cc;&quot;&gt;1&lt;/span&gt;
    &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;else&lt;/span&gt; choose&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#40;&lt;/span&gt;r&lt;span style=&quot;color: #000066;&quot;&gt;-&lt;/span&gt;&lt;span style=&quot;color: #cc66cc;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #000066;&quot;&gt;,&lt;/span&gt;k&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#41;&lt;/span&gt; &lt;span style=&quot;color: #000066;&quot;&gt;+&lt;/span&gt; choose&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#40;&lt;/span&gt;r&lt;span style=&quot;color: #000066;&quot;&gt;-&lt;/span&gt;&lt;span style=&quot;color: #cc66cc;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #000066;&quot;&gt;,&lt;/span&gt;k&lt;span style=&quot;color: #000066;&quot;&gt;-&lt;/span&gt;&lt;span style=&quot;color: #cc66cc;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#41;&lt;/span&gt;
&amp;nbsp;
fun pascal_triangle&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#40;&lt;/span&gt;x &lt;span style=&quot;color: #000066;&quot;&gt;:&lt;/span&gt; int&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#41;&lt;/span&gt; &lt;span style=&quot;color: #000066;&quot;&gt;=&lt;/span&gt;
    &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;if&lt;/span&gt; &lt;span style=&quot;color: #009900;&quot;&gt;&amp;#40;&lt;/span&gt;x &amp;lt;&lt;span style=&quot;color: #000066;&quot;&gt;=&lt;/span&gt;&lt;span style=&quot;color: #cc66cc;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#41;&lt;/span&gt; &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;then&lt;/span&gt; &lt;span style=&quot;color: #009900;&quot;&gt;&amp;#91;&lt;/span&gt;&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#91;&lt;/span&gt;&lt;span style=&quot;color: #cc66cc;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#93;&lt;/span&gt;&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#93;&lt;/span&gt;
    &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;else&lt;/span&gt;
        let fun count &lt;span style=&quot;color: #009900;&quot;&gt;&amp;#40;&lt;/span&gt;from&lt;span style=&quot;color: #000066;&quot;&gt;:&lt;/span&gt;int&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#41;&lt;/span&gt; &lt;span style=&quot;color: #000066;&quot;&gt;=&lt;/span&gt;
            &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;if&lt;/span&gt; from&lt;span style=&quot;color: #000066;&quot;&gt;=&lt;/span&gt;x &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;then&lt;/span&gt; &lt;span style=&quot;color: #cc66cc;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #000066;&quot;&gt;::&lt;/span&gt;&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#91;&lt;/span&gt;&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#93;&lt;/span&gt; &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;else&lt;/span&gt; choose&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#40;&lt;/span&gt;x&lt;span style=&quot;color: #000066;&quot;&gt;,&lt;/span&gt;from&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#41;&lt;/span&gt; &lt;span style=&quot;color: #000066;&quot;&gt;::&lt;/span&gt; count&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#40;&lt;/span&gt;from&lt;span style=&quot;color: #000066;&quot;&gt;+&lt;/span&gt;&lt;span style=&quot;color: #cc66cc;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#41;&lt;/span&gt;            
        &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;in&lt;/span&gt; count&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#40;&lt;/span&gt;&lt;span style=&quot;color: #cc66cc;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#41;&lt;/span&gt;&lt;span style=&quot;color: #000066;&quot;&gt;::&lt;/span&gt;pascal_triangle&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#40;&lt;/span&gt;x&lt;span style=&quot;color: #000066;&quot;&gt;-&lt;/span&gt;&lt;span style=&quot;color: #cc66cc;&quot;&gt;1&lt;/span&gt;&lt;span style=&quot;color: #009900;&quot;&gt;&amp;#41;&lt;/span&gt;
    &lt;span style=&quot;color: #000000; font-weight: bold;&quot;&gt;end&lt;/span&gt;&lt;/pre&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;*Note while the “choose” function could have been written as nested function inside “pascal_triangle” (like the “count” function), for the reasons of simplicity and comprehension I have put is as separate code. Same goes for the parenthesis around attributes, which are not required by the compiler.&lt;/p&gt;

</description>
        <pubDate>Sat, 26 Jan 2013 00:00:00 -0500</pubDate>
        <link>http://iamaaditya.github.io/2013/01/pascal-triangle-in-sml/</link>
        <guid isPermaLink="true">http://iamaaditya.github.io/2013/01/pascal-triangle-in-sml/</guid>
      </item>
    
  </channel>
</rss>